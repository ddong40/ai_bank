"""
ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ë¹„ì •ìƒ ì‘ë™ ë¶„ë¥˜ ëª¨ë¸ (ìµœì í™”ëœ ë”¥ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸)

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. ì „ì²˜ë¦¬: MinMaxScaler â†’ QuantileTransformer(normal) ë³€ê²½ìœ¼ë¡œ ë¶„í¬ ì •ê·œí™”
2. ëª¨ë¸ êµ¬ì¡°: Flatten ì œê±°, CLS í† í° + Attention Pooling ë„ì…ìœ¼ë¡œ ê¸€ë¡œë²Œ ì •ë³´ ì§‘ì•½
3. ìµœì í™”: AdamW(3e-4) + CosineDecay(Warmup) + Weight Decay(1e-4) + Gradient Clipping(1.0)
4. ê²€ì¦: StratifiedKFold OOF(Out-of-Fold) êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì ì¸ ëª¨ë¸ ì„ íƒ
5. ì•™ìƒë¸”: LightGBM OOF + Neural Network OOF ìŠ¤íƒœí‚¹ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í–¥ìƒ

ëª¨ë¸ êµ¬ì¡°:
- Dual Branch Dilated Conv1D + Positional Encoding + Multi-Head Attention
- CLS í† í° ê¸°ë°˜ Attention Poolingìœ¼ë¡œ ì‹œí€€ìŠ¤ ì •ë³´ ì§‘ì•½
- Pure Conv1D ë¶„ë¥˜ í—¤ë“œ (Dense layer ì™„ì „ ì œê±°)
- 21ê°œ í´ë˜ìŠ¤ ë‹¤ì¤‘ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ë”¥ëŸ¬ë‹ ë° ì „ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, BatchNormalization, Dropout, GlobalAveragePooling1D,
    Conv1D, MaxPooling1D, MultiHeadAttention, LayerNormalization,
    Reshape, Concatenate, Add
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import CosineDecay

# TensorFlow ë²„ì „ì— ë”°ë¥¸ AdamW import ì‹œë„
try:
    from tensorflow.keras.optimizers.experimental import AdamW
except ImportError:
    try:
        from tensorflow.keras.optimizers import AdamW
    except ImportError:
        # AdamWê°€ ì—†ëŠ” ê²½ìš° weight_decay ì—†ì´ Adam ì‚¬ìš©
        print("âš ï¸ AdamWë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. weight_decay ì—†ì´ Adamì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        AdamW = None
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.losses import SparseCategoricalCrossentropy

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import QuantileTransformer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.utils.class_weight import compute_class_weight
import lightgbm as lgb
from catboost import CatBoostClassifier, Pool
from sklearn.decomposition import PCA
from scipy.stats import rankdata
import matplotlib.pyplot as plt
import seaborn as sns

# ì‹œë“œ ê³ ì •
tf.random.set_seed(42)
np.random.seed(42)

# GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš© (GPU ì‚¬ìš© ì‹œ)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

print("=" * 60)
print("Pure Conv1D ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ë¹„ì •ìƒ ì‘ë™ ë¶„ë¥˜ ëª¨ë¸ (Dense layer ì œê±°)")
print("=" * 60)

# 1. ë°ì´í„° ì ì¬ ë° ë¶„ë¦¬
print("\n1. ë°ì´í„° ì ì¬ ì¤‘...")
train_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/train.csv')
test_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/test.csv')
submission_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/sample_submission.csv')

# X, y ë¶„ë¦¬
X = train_df.drop(columns=['target', 'ID'])
y = train_df['target']  # ì •ìˆ˜ ë¼ë²¨ ê·¸ëŒ€ë¡œ ìœ ì§€ (0~20)

print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"í”¼ì²˜ ìˆ˜: {X.shape[1]}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {len(y.unique())}")
print(f"í´ë˜ìŠ¤ ë¶„í¬:\n{y.value_counts().sort_index()}")

# íŠ¹ì§• ê³µí•™ í•¨ìˆ˜ ì¶”ê°€
def add_features(df):
    """í†µê³„ì  íŠ¹ì§•ê³¼ PCA íŠ¹ì§• ì¶”ê°€"""
    num = df.select_dtypes(include=[np.number]).copy()
    df2 = df.copy()
    
    # í–‰ë³„ í†µê³„ íŠ¹ì§•
    df2['row_mean'] = num.mean(axis=1)
    df2['row_std']  = num.std(axis=1)
    df2['row_max']  = num.max(axis=1)
    df2['row_min']  = num.min(axis=1)
    df2['row_q25']  = num.quantile(0.25, axis=1)
    df2['row_q75']  = num.quantile(0.75, axis=1)
    df2['row_skew'] = num.skew(axis=1)
    df2['row_kurt'] = num.kurtosis(axis=1)
    
    print(f"   âœ… í†µê³„ì  íŠ¹ì§• 8ê°œ ì¶”ê°€ ì™„ë£Œ")
    return df2

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
X_test = test_df.drop(columns=['ID'])

# íŠ¹ì§• ê³µí•™ ì ìš©
print("\nğŸ”§ íŠ¹ì§• ê³µí•™ ì ìš© ì¤‘...")
X = add_features(X)
X_test = add_features(X_test)

# PCA íŠ¹ì§• ì¶”ê°€ (ì›ë³¸ íŠ¹ì§•ì— ì¶”ê°€)
print("   ğŸ” PCA íŠ¹ì§• ì¶”ê°€ ì¤‘...")
pca = PCA(n_components=16, random_state=42)
pca_train = pca.fit_transform(X.select_dtypes(include=[np.number]))
pca_test = pca.transform(X_test.select_dtypes(include=[np.number]))

for i in range(pca_train.shape[1]):
    X[f'pca_{i}'] = pca_train[:, i]
    X_test[f'pca_{i}'] = pca_test[:, i]

print(f"   âœ… PCA íŠ¹ì§• 16ê°œ ì¶”ê°€ ì™„ë£Œ")
print(f"   ğŸ“Š ìµœì¢… íŠ¹ì§• ìˆ˜: {X.shape[1]}ê°œ (ì›ë³¸: 52ê°œ â†’ í™•ì¥: {X.shape[1]}ê°œ)")

print(f"í´ë˜ìŠ¤ ë¶„í¬:\n{y.value_counts().sort_index()}")

# 2. StratifiedKFold OOF ì¤€ë¹„
print("\n2. StratifiedKFold OOF ê²€ì¦ ì¤€ë¹„ ì¤‘...")
N_FOLDS = 5
skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

print(f"StratifiedKFold: {N_FOLDS}ê°œ í´ë“œë¡œ êµì°¨ê²€ì¦")
print(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {X.shape}")

# OOF ì˜ˆì¸¡ê°’ ì €ì¥ì„ ìœ„í•œ ë°°ì—´ ì´ˆê¸°í™”
oof_predictions = np.zeros((len(X), 21))  # 21ê°œ í´ë˜ìŠ¤ í™•ë¥ 
test_predictions_nn = np.zeros((len(test_df), 21))  # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’
fold_scores = []

# í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ class_weight ê³„ì‚° (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
print("\nğŸ“Š í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°...")
class_weights = compute_class_weight(
    'balanced', 
    classes=np.unique(y), 
    y=y
)
class_weight_dict = dict(zip(np.unique(y), class_weights))

print("í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:")
for cls, weight in class_weight_dict.items():
    count = (y == cls).sum()
    print(f"  í´ë˜ìŠ¤ {cls}: ê°€ì¤‘ì¹˜ {weight:.3f} (ìƒ˜í”Œ ìˆ˜: {count})")

print(f"\nê°€ì¤‘ì¹˜ ë²”ìœ„: {min(class_weights):.3f} ~ {max(class_weights):.3f}")
print("â†’ Macro F1 Score ìµœì í™”ë¥¼ ìœ„í•´ ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬")

# 3. ì´ìƒì¹˜ í´ë¦¬í•‘ (ì™„í™” ì²˜ë¦¬)
print("\n3. IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ ì ìš© ì¤‘...")

# 4. ì»¤ìŠ¤í…€ F1 Score ë©”íŠ¸ë¦­ ì •ì˜
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=21, average='macro', name='f1_score', **kwargs):
        super().__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.average = average
        self.f1_score = self.add_weight(name='f1', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
        
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.argmax(y_pred, axis=-1)
        y_true = tf.cast(y_true, y_pred.dtype)
        
        # F1 score ê³„ì‚°ì„ ìœ„í•œ confusion matrix
        f1_value = tf.py_function(
            func=self._compute_f1,
            inp=[y_true, y_pred],
            Tout=tf.float32
        )
        
        self.f1_score.assign_add(f1_value)
        self.count.assign_add(1.0)
    
    def _compute_f1(self, y_true, y_pred):
        return f1_score(y_true.numpy(), y_pred.numpy(), average=self.average, zero_division=0)
    
    def result(self):
        return self.f1_score / self.count
    
    def reset_state(self):
        self.f1_score.assign(0.0)
        self.count.assign(0.0)

# AdamW ì˜µí‹°ë§ˆì´ì € ì„¤ì • í•¨ìˆ˜ ì •ì˜
def create_optimizer_with_warmup(initial_learning_rate=3e-4, decay_steps=1000, warmup_steps=100):
    """AdamW ì˜µí‹°ë§ˆì´ì € ìƒì„± (ê³ ì • í•™ìŠµë¥ )"""
    
    # AdamWê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
    if AdamW is not None:
        print("âœ… AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (Weight Decay í¬í•¨)")
        optimizer = AdamW(
            learning_rate=initial_learning_rate,  # ê³ ì • í•™ìŠµë¥ 
            weight_decay=1e-4,  # L2 ì •ê·œí™”
            clipnorm=1.0,       # Gradient Clipping
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
    else:
        # AdamWê°€ ì—†ëŠ” ê²½ìš° Adam ì‚¬ìš© (clipnormë§Œ ì ìš©)
        print("âš ï¸  Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (Weight Decay ì—†ìŒ)")
        optimizer = Adam(
            learning_rate=initial_learning_rate,  # ê³ ì • í•™ìŠµë¥ 
            clipnorm=1.0,       # Gradient Clipping
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
    
    return optimizer

# 5. ëª¨ë¸ êµ¬ì„± (Conv1D + Positional Encoding + Transformer)
print("\n5. Conv1D + Positional Encoding + Transformer ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì„± ì¤‘...")

class PositionalEncoding(tf.keras.layers.Layer):
    """
    Transformer ë…¼ë¬¸ì˜ Sinusoidal Positional Encodingì„ Keras Layerë¡œ êµ¬í˜„
    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    
    def __init__(self, max_len=1000, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.max_len = max_len
        
    def get_angles(self, pos, i, d_model):
        """ê°ë„ ê³„ì‚° í•¨ìˆ˜"""
        angle_rates = 1 / tf.pow(10000.0, tf.cast(2 * (i // 2), tf.float32) / tf.cast(d_model, tf.float32))
        return pos * angle_rates
    
    def call(self, inputs):
        seq_len = tf.shape(inputs)[1]
        d_model = tf.shape(inputs)[2]
        
        # ìœ„ì¹˜ ì¸ë±ìŠ¤ ìƒì„± (0, 1, 2, ..., seq_len-1)
        pos = tf.cast(tf.range(seq_len), tf.float32)[:, tf.newaxis]
        
        # ì°¨ì› ì¸ë±ìŠ¤ ìƒì„± (0, 1, 2, ..., d_model-1)
        i = tf.cast(tf.range(d_model), tf.float32)[tf.newaxis, :]
        
        # ê°ë„ ê³„ì‚°
        angle_rads = self.get_angles(pos, i, d_model)
        
        # ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin, í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cos ì ìš©
        sines = tf.sin(angle_rads[:, 0::2])
        cosines = tf.cos(angle_rads[:, 1::2])
        
        # sinê³¼ cosë¥¼ ë²ˆê°ˆì•„ê°€ë©° ë°°ì¹˜
        pos_encoding = tf.concat([sines, cosines], axis=-1)
        
        # d_modelì´ í™€ìˆ˜ì¸ ê²½ìš° ë§ˆì§€ë§‰ ì°¨ì› ì¡°ì •
        if d_model % 2 == 1:
            pos_encoding = pos_encoding[:, :-1]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        pos_encoding = pos_encoding[tf.newaxis, :, :]
        
        # ì…ë ¥ê³¼ positional encoding í•©ì‚°
        return inputs + pos_encoding
    
    def get_config(self):
        config = super().get_config()
        config.update({"max_len": self.max_len})
        return config

def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    """Transformer ë¸”ë¡ êµ¬ì„± í•¨ìˆ˜"""
    # Multi-Head Attention
    attention_output = MultiHeadAttention(
        num_heads=num_heads, 
        key_dim=head_size,
        dropout=dropout
    )(inputs, inputs)
    
    # Add & Norm 1
    attention_output = Dropout(dropout)(attention_output)
    x1 = Add()([inputs, attention_output])
    x1 = LayerNormalization(epsilon=1e-6)(x1)
    
    # Feed Forward Network
    ffn_output = Dense(ff_dim, activation='relu')(x1)
    ffn_output = Dropout(dropout)(ffn_output)
    ffn_output = Dense(inputs.shape[-1])(ffn_output)
    
    # Add & Norm 2
    ffn_output = Dropout(dropout)(ffn_output)
    x2 = Add()([x1, ffn_output])
    x2 = LayerNormalization(epsilon=1e-6)(x2)
    
    return x2

class CLSTokenLayer(tf.keras.layers.Layer):
    """CLS í† í°ì„ ì‹œí€€ìŠ¤ ì•ì— ì¶”ê°€í•˜ëŠ” ë ˆì´ì–´"""
    
    def __init__(self, embed_dim, **kwargs):
        super(CLSTokenLayer, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        
    def build(self, input_shape):
        # CLS í† í°ì„ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì´ˆê¸°í™”
        self.cls_token = self.add_weight(
            name='cls_token',
            shape=(1, 1, self.embed_dim),
            initializer='random_normal',
            trainable=True
        )
        super().build(input_shape)
        
    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        # CLS í† í°ì„ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë³µì œ
        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])
        # CLS í† í°ì„ ì‹œí€€ìŠ¤ ì•ì— ì¶”ê°€
        return tf.concat([cls_tokens, inputs], axis=1)
    
    def get_config(self):
        config = super().get_config()
        config.update({"embed_dim": self.embed_dim})
        return config

class AttentionPooling(tf.keras.layers.Layer):
    """Attention ê¸°ë°˜ í’€ë§ìœ¼ë¡œ CLS í† í°ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë ˆì´ì–´"""
    
    def __init__(self, embed_dim, **kwargs):
        super(AttentionPooling, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        
    def build(self, input_shape):
        # Attention weightsë¥¼ ìƒì„±í•˜ëŠ” Dense layer
        self.attention_dense = tf.keras.layers.Dense(
            1, use_bias=False, name='attention_weights'
        )
        super().build(input_shape)
        
    def call(self, inputs):
        # inputs shape: (batch_size, seq_len, embed_dim)
        # CLS í† í°ì€ ì²« ë²ˆì§¸ ìœ„ì¹˜ (index 0)ì— ìˆìŒ
        cls_output = inputs[:, 0, :]  # (batch_size, embed_dim)
        
        # ëª¨ë“  í† í°ì— ëŒ€í•œ attention ê³„ì‚°
        attention_scores = self.attention_dense(inputs)  # (batch_size, seq_len, 1)
        attention_weights = tf.nn.softmax(attention_scores, axis=1)  # (batch_size, seq_len, 1)
        
        # Attention weighted average
        context_vector = tf.reduce_sum(inputs * attention_weights, axis=1)  # (batch_size, embed_dim)
        
        # CLS í† í°ê³¼ context vectorë¥¼ ê²°í•©
        combined = cls_output + context_vector
        return combined
    
    def get_config(self):
        config = super().get_config()
        config.update({"embed_dim": self.embed_dim})
        return config

def create_dual_branch_model(input_dim, num_classes=21):
    """ë“€ì–¼ ë¸Œëœì¹˜ Dilated Conv1D + CLS í† í° + Attention Pooling ëª¨ë¸"""
    
    # ì…ë ¥ì¸µ (ë™ì¼í•œ ë°ì´í„°ê°€ ë‘ ë¸Œëœì¹˜ë¡œ ë¶„ê¸°)
    inputs = Input(shape=(input_dim,))
    
    # 1D ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ (52 í”¼ì²˜ â†’ 52 ì‹œê°„ìŠ¤í…)
    reshaped = Reshape((input_dim, 1))(inputs)
    
    # ===== X ë¸Œëœì¹˜: Dilated Conv1Dë¡œ ì„¸ë°€í•œ ì‹œê°„ì  íŒ¨í„´ ì¶”ì¶œ =====
    # ì‘ì€ dilation rateë¡œ ë¡œì»¬ íŒ¨í„´ì— ì§‘ì¤‘
    x = Conv1D(filters=64, kernel_size=3, dilation_rate=1, padding='same', name='x_dilconv1')(reshaped)
    x = tf.keras.layers.ReLU(name='x_relu1')(x)
    x = LayerNormalization(epsilon=1e-6, name='x_ln1')(x)
    x = Dropout(0.1, name='x_dropout1')(x)
    
    x = Conv1D(filters=32, kernel_size=3, dilation_rate=2, padding='same', name='x_dilconv2')(x)
    x = tf.keras.layers.ReLU(name='x_relu2')(x)
    x = LayerNormalization(epsilon=1e-6, name='x_ln2')(x)
    x = MaxPooling1D(pool_size=2, padding='same', name='x_pool')(x)  # 26 timesteps
    x = Dropout(0.1, name='x_dropout2')(x)
    
    # ===== Z ë¸Œëœì¹˜: Dilated Conv1Dë¡œ ì¥ê±°ë¦¬ ì‹œê°„ì  ì˜ì¡´ì„± í¬ì°© =====
    # í° dilation rateë¡œ ê¸€ë¡œë²Œ íŒ¨í„´ì— ì§‘ì¤‘
    z = Conv1D(filters=32, kernel_size=3, dilation_rate=4, padding='same', name='z_dilconv1')(reshaped)
    z = tf.keras.layers.ReLU(name='z_relu1')(z)
    z = LayerNormalization(epsilon=1e-6, name='z_ln1')(z)
    z = Dropout(0.1, name='z_dropout1')(z)
    
    z = Conv1D(filters=16, kernel_size=3, dilation_rate=8, padding='same', name='z_dilconv2')(z)
    z = tf.keras.layers.ReLU(name='z_relu2')(z)
    z = LayerNormalization(epsilon=1e-6, name='z_ln2')(z)
    z = MaxPooling1D(pool_size=2, padding='same', name='z_pool')(z)  # 26 timesteps
    z = Dropout(0.1, name='z_dropout2')(z)
    
    # ===== ë¸Œëœì¹˜ ê²°í•© =====
    # x: (batch, 26, 32), z: (batch, 26, 16) â†’ concat â†’ (batch, 26, 48)
    combined = Concatenate(axis=-1, name='branch_concat')([x, z])
    
    print(f"ë¸Œëœì¹˜ ê²°í•© í›„ shape: {combined.shape}")
    
    # ===== CLS í† í° ì¶”ê°€ =====
    # ë¸Œëœì¹˜ ê²°í•© í›„ íŠ¹ì§• ì°¨ì›: 48
    cls_layer = CLSTokenLayer(embed_dim=48, name='cls_token_layer')
    combined_with_cls = cls_layer(combined)  # (batch, 27, 48) - CLS í† í° ì¶”ê°€ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ +1
    
    print(f"CLS í† í° ì¶”ê°€ í›„ shape: {combined_with_cls.shape}")
    
    # ===== Positional Encoding ì¶”ê°€ =====
    # Transformer ë…¼ë¬¸ê³¼ ë™ì¼í•œ sin/cos ê¸°ë°˜ í¬ì§€ì…”ë„ ì¸ì½”ë”© ì ìš©
    pos_encoder = PositionalEncoding(name='positional_encoding')
    combined_with_pos = pos_encoder(combined_with_cls)
    
    # ===== Transformer ë¸”ë¡ (CLS í† í° í¬í•¨ ê¸€ë¡œë²Œ ê´€ê³„ í•™ìŠµ) =====
    transformer_out = transformer_block(
        inputs=combined_with_pos,  # CLS í† í° + í¬ì§€ì…”ë„ ì¸ì½”ë”©ì´ ì¶”ê°€ëœ ì…ë ¥ ì‚¬ìš©
        head_size=16,      # 48 ì±„ë„ì— ë§ê²Œ ì¡°ì •
        num_heads=3,       # 3ê°œ í—¤ë“œ ìœ ì§€
        ff_dim=96,         # 48 * 2
        dropout=0.1
    )
    
    # ===== Attention Poolingìœ¼ë¡œ CLS í† í° ì •ë³´ ì¶”ì¶œ =====
    # GlobalAveragePooling1D ëŒ€ì‹  CLS í† í°ê³¼ Attention Pooling ì‚¬ìš©
    attention_pooled = AttentionPooling(embed_dim=48, name='attention_pooling')(transformer_out)  # (batch, 48)
    
    print(f"Attention Pooling í›„ shape: {attention_pooled.shape}")
    
    # ===== Conv1D Classification Head (1D íŠ¹ì§• ë²¡í„° ì²˜ë¦¬) =====
    # Attention pooled ì¶œë ¥ì„ 1D ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ Conv1D ì ìš©
    # (batch, 48) â†’ (batch, 48, 1)ë¡œ ë³€í™˜í•˜ì—¬ Conv1D ì ìš© ê°€ëŠ¥í•˜ë„ë¡ í•¨
    clf_input = tf.expand_dims(attention_pooled, axis=-1)  # (batch, 48, 1)
    
    # ì²« ë²ˆì§¸ Conv1D: ì±„ë„ í™•ì¥ ë° íŠ¹ì§• ì¶”ì¶œ
    clf = Conv1D(128, kernel_size=3, padding='same', activation='relu', name='clf_conv1')(clf_input)
    clf = LayerNormalization(epsilon=1e-6, name='clf_ln1')(clf)
    clf = Dropout(0.2, name='clf_dropout1')(clf)
    
    # ë‘ ë²ˆì§¸ Conv1D: ì±„ë„ ê°ì†Œí•˜ë©° ê³ ìˆ˜ì¤€ íŠ¹ì§• ì¶”ì¶œ
    clf = Conv1D(64, kernel_size=3, padding='same', activation='relu', name='clf_conv2')(clf)
    clf = LayerNormalization(epsilon=1e-6, name='clf_ln2')(clf)
    clf = Dropout(0.2, name='clf_dropout2')(clf)
    
    # ì„¸ ë²ˆì§¸ Conv1D: ìµœì¢… íŠ¹ì§• ì••ì¶•
    clf = Conv1D(32, kernel_size=3, padding='same', activation='relu', name='clf_conv3')(clf)
    clf = LayerNormalization(epsilon=1e-6, name='clf_ln3')(clf)
    clf = Dropout(0.1, name='clf_dropout3')(clf)
    
    # Conv1Dë¡œ ìµœì¢… í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ ì±„ë„ ìƒì„±
    clf = Conv1D(num_classes, kernel_size=1, padding='same', name='clf_conv_output')(clf)  # (batch, 48, 21)
    
    # GlobalAveragePooling1Dë¡œ ì‹œí€€ìŠ¤ ì°¨ì› ì¶•ì†Œí•˜ì—¬ ìµœì¢… ì¶œë ¥
    clf = GlobalAveragePooling1D(name='clf_gap')(clf)  # (batch, 21)
    
    # Softmax í™œì„±í™”ë¡œ í™•ë¥  ë¶„í¬ ìƒì„±
    outputs = tf.keras.layers.Softmax(name='output')(clf)
    
    model = Model(inputs=inputs, outputs=outputs, name='DualBranchConvTransformer')
    return model

# =============== ë”¥ëŸ¬ë‹ ëª¨ë¸ ì£¼ì„ì²˜ë¦¬ (íŠ¸ë¦¬ê³„ì—´ë§Œ ì‹¤í–‰) ===============
# # StratifiedKFold OOF í•™ìŠµ ì‹œì‘
# print("\nğŸš€ StratifiedKFold OOF í•™ìŠµ ì‹œì‘!")
# print(f"   ğŸ“Š ì´ {N_FOLDS}ê°œ í´ë“œ êµì°¨ê²€ì¦")
# print(f"   ğŸ¯ ëª©í‘œ: 21ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)")
# print(f"   âš–ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ì ìš© (ë¶ˆê· í˜• í•´ê²°)")
# print("=" * 60)

# for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
#     print(f"\nğŸ“‹ Fold {fold + 1}/{N_FOLDS} ì‹œì‘...")
    
#     # í´ë“œë³„ ë°ì´í„° ë¶„í• 
#     X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
#     y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
    
#     print(f"   ğŸ”¸ í›ˆë ¨: {len(X_fold_train)}ê°œ, ê²€ì¦: {len(X_fold_val)}ê°œ")
    
#     # 3. í´ë“œë³„ ì „ì²˜ë¦¬ (IQR í´ë¦¬í•‘ + QuantileTransformer)
#     def apply_fold_preprocessing(X_train, X_val, X_test):
#         """í´ë“œë³„ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
#         # IQR í´ë¦¬í•‘
#         X_train_clipped = X_train.copy()
#         X_val_clipped = X_val.copy()
#         X_test_clipped = X_test.copy()
        
#         for column in X_train.columns:
#             Q1 = X_train[column].quantile(0.25)
#             Q3 = X_train[column].quantile(0.75)
#             IQR = Q3 - Q1
            
#             # í›ˆë ¨/ê²€ì¦ìš© ê²½ê³„ê°’
#             train_lower = Q1 - 1.5 * IQR
#             train_upper = Q3 + 1.5 * IQR
            
#             # í…ŒìŠ¤íŠ¸ìš© ê²½ê³„ê°’ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
#             test_lower = Q1 - 3.0 * IQR
#             test_upper = Q3 + 3.0 * IQR
            
#             # í´ë¦¬í•‘ ì ìš© (ì‹ ê²½ë§ìš©ë§Œ - íŠ¸ë¦¬ëª¨ë¸ì€ ì›ë³¸ ì‚¬ìš©)
#             X_train_clipped[column] = X_train[column].clip(train_lower, train_upper)
#             X_val_clipped[column] = X_val[column].clip(train_lower, train_upper)
#             X_test_clipped[column] = X_test[column].clip(test_lower, test_upper)
        
#         # QuantileTransformer ì •ê·œí™”
#         scaler = QuantileTransformer(output_distribution='normal', random_state=42)
#         X_train_scaled = scaler.fit_transform(X_train_clipped)
#         X_val_scaled = scaler.transform(X_val_clipped)
#         X_test_scaled = scaler.transform(X_test_clipped)
#         
#         return X_train_scaled, X_val_scaled, X_test_scaled
    
#     # í´ë“œë³„ ì „ì²˜ë¦¬ ì ìš©
#     X_fold_train_scaled, X_fold_val_scaled, X_test_scaled = apply_fold_preprocessing(
#         X_fold_train, X_fold_val, X_test
#     )
    
#     # 4. ëª¨ë¸ ìƒì„± (í´ë“œë§ˆë‹¤ ìƒˆë¡œ ìƒì„±)
#     input_dim = X_fold_train_scaled.shape[1]
#     model = create_dual_branch_model(input_dim, num_classes=21)
    
#     # 5. ëª¨ë¸ ì»´íŒŒì¼ (AdamW + CosineDecay)
#     optimizer = create_optimizer_with_warmup(
#         initial_learning_rate=3e-4,
#         decay_steps=1000,
#         warmup_steps=100
#     )
    
#     model.compile(
#         optimizer=optimizer,
#         loss=SparseCategoricalCrossentropy(),
#         metrics=['accuracy', F1Score(num_classes=21, average='macro')]
#     )
    
#     # 6. ì½œë°± ì„¤ì •
#     early_stopping = EarlyStopping(
#         monitor='val_f1_score',
#         mode='max',
#         patience=15,
#         restore_best_weights=True,
#         verbose=0
#     )
    
#     reduce_lr = ReduceLROnPlateau(
#         monitor='val_f1_score',
#         mode='max',
#         factor=0.5,
#         patience=5,
#         min_lr=1e-7,
#         verbose=0
#     )
    
#     # 7. ëª¨ë¸ í•™ìŠµ
#     print(f"   ğŸš€ Fold {fold + 1} í•™ìŠµ ì‹œì‘...")
    
#     history = model.fit(
#         X_fold_train_scaled, y_fold_train,
#         validation_data=(X_fold_val_scaled, y_fold_val),
#         epochs=200,
#         batch_size=128,
#         class_weight=class_weight_dict,
#         callbacks=[early_stopping, reduce_lr],
#         verbose=1  # í”„ë¡œê·¸ë˜ìŠ¤ ë°” í‘œì‹œ
#     )
    
#     # 8. OOF ì˜ˆì¸¡ ì €ì¥
#     val_pred = model.predict(X_fold_val_scaled, verbose=0)
#     oof_predictions[val_idx] = val_pred
    
#     # 9. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ëˆ„ì 
#     test_pred = model.predict(X_test_scaled, verbose=0)
#     test_predictions_nn += test_pred / N_FOLDS
    
#     # 10. í´ë“œ ì„±ëŠ¥ í‰ê°€
#     val_pred_classes = np.argmax(val_pred, axis=1)
#     fold_f1 = f1_score(y_fold_val, val_pred_classes, average='macro')
#     fold_acc = accuracy_score(y_fold_val, val_pred_classes)
#     fold_scores.append(fold_f1)
    
#     print(f"   âœ… Fold {fold + 1} ì™„ë£Œ!")
#     print(f"      ğŸ“Š ê²€ì¦ F1 Score: {fold_f1:.4f}")
#     print(f"      ğŸ“Š ê²€ì¦ Accuracy: {fold_acc:.4f}")
#     print(f"      ğŸ“Š í•™ìŠµ ì—í¬í¬: {len(history.history['loss'])}")
#     print(f"      ğŸ“Š ìµœê³  ê²€ì¦ F1: {max(history.history['val_f1_score']):.4f}")

# # OOF ì „ì²´ ì„±ëŠ¥ í‰ê°€
# oof_pred_classes = np.argmax(oof_predictions, axis=1)
# oof_f1 = f1_score(y, oof_pred_classes, average='macro')
# oof_acc = accuracy_score(y, oof_pred_classes)

# print(f"\nğŸ‰ StratifiedKFold OOF í•™ìŠµ ì™„ë£Œ!")
# print(f"   ğŸ“Š í‰ê·  CV F1 Score: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}")
# print(f"   ğŸ“Š OOF F1 Score: {oof_f1:.4f}")
# print(f"   ğŸ“Š OOF Accuracy: {oof_acc:.4f}")
# print(f"   ğŸ“Š í´ë“œë³„ F1 ì ìˆ˜: {[f'{score:.4f}' for score in fold_scores]}")
# print("=" * 60)

# ë”¥ëŸ¬ë‹ ëª¨ë¸ OOF ì˜ˆì¸¡ê°’ ì´ˆê¸°í™” (ë”ë¯¸ ë°ì´í„°)
print("\nâš ï¸  ë”¥ëŸ¬ë‹ ëª¨ë¸ ì£¼ì„ì²˜ë¦¬ë¨ - íŠ¸ë¦¬ê³„ì—´ ëª¨ë¸ë§Œ ì‹¤í–‰")
oof_predictions = np.zeros((len(X), 21))  # ë”ë¯¸ ë°ì´í„°
test_predictions_nn = np.zeros((len(test_df), 21))  # ë”ë¯¸ ë°ì´í„°
oof_f1 = 0.0  # ë”ë¯¸ ì ìˆ˜

# LightGBM Macro F1 ì»¤ìŠ¤í…€ í‰ê°€ í•¨ìˆ˜
def lgb_macro_f1(preds, train_data):
    """LightGBMìš© Macro F1 Score í‰ê°€ í•¨ìˆ˜"""
    y_true = train_data.get_label().astype(int)
    preds = preds.reshape(21, -1).T  # (n_samples, 21)
    y_pred = np.argmax(preds, axis=1)
    f1 = f1_score(y_true, y_pred, average='macro')
    return 'macro_f1', f1, True  # True: higher is better

# LightGBM OOF í•™ìŠµ (ê°•í™”ëœ íŒŒë¼ë¯¸í„°)
print("\nğŸŒŸ LightGBM OOF í•™ìŠµ ì‹œì‘ (Macro F1 ìµœì í™”)...")

# LightGBMìš© OOF ì˜ˆì¸¡ê°’ ì €ì¥ ë°°ì—´
oof_predictions_lgb = np.zeros((len(X), 21))
test_predictions_lgb = np.zeros((len(test_df), 21))
lgb_fold_scores = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\nğŸ“‹ LightGBM Fold {fold + 1}/{N_FOLDS} ì‹œì‘...")
    
    # í´ë“œë³„ ë°ì´í„° ë¶„í• 
    X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
    y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
    
    # LightGBM ê°•í™”ëœ íŒŒë¼ë¯¸í„° (Macro F1 ìµœì í™”)
    lgb_params = {
        'objective': 'multiclass',
        'num_class': 21,
        'boosting_type': 'gbdt',
        'learning_rate': 0.03,      # ë” ì„¸ë°€í•œ í•™ìŠµ
        'num_leaves': 128,          # í‘œí˜„ë ¥ ì¦ê°€
        'min_data_in_leaf': 64,     # ê³¼ì í•© ì œì–´
        'max_depth': -1,
        'feature_fraction': 0.9,    # íŠ¹ì§• ë‹¤ì–‘ì„±
        'bagging_fraction': 0.9,
        'bagging_freq': 1,
        'reg_alpha': 2.0,           # L1 ì •ê·œí™” ê°•í™”
        'reg_lambda': 10.0,         # L2 ì •ê·œí™” ê°•í™”
        'verbosity': -1,
        'seed': 42,
        'class_weight': None        # ì»¤ìŠ¤í…€ F1ìœ¼ë¡œ ì²˜ë¦¬
    }
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_fold_train, label=y_fold_train)
    val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)
    
    # ëª¨ë¸ í•™ìŠµ (Macro F1 ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ)
    lgb_model = lgb.train(
        lgb_params,
        train_data,
        valid_sets=[train_data, val_data],
        feval=lgb_macro_f1,              # Macro F1 ì»¤ìŠ¤í…€ í‰ê°€
        num_boost_round=2000,            # ì¶©ë¶„í•œ rounds
        callbacks=[
            lgb.early_stopping(200),      # Macro F1 ê¸°ì¤€ ì¡°ê¸° ì¢…ë£Œ
            lgb.log_evaluation(0)
        ]
    )
    
    # OOF ì˜ˆì¸¡
    val_pred_lgb = lgb_model.predict(X_fold_val, num_iteration=lgb_model.best_iteration)
    oof_predictions_lgb[val_idx] = val_pred_lgb
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ëˆ„ì 
    test_pred_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)
    test_predictions_lgb += test_pred_lgb / N_FOLDS
    
    # í´ë“œ ì„±ëŠ¥ í‰ê°€
    val_pred_classes_lgb = np.argmax(val_pred_lgb, axis=1)
    fold_f1_lgb = f1_score(y_fold_val, val_pred_classes_lgb, average='macro')
    fold_acc_lgb = accuracy_score(y_fold_val, val_pred_classes_lgb)
    lgb_fold_scores.append(fold_f1_lgb)
    
    print(f"   âœ… LightGBM Fold {fold + 1} ì™„ë£Œ!")
    print(f"      ğŸ“Š ê²€ì¦ F1 Score: {fold_f1_lgb:.4f}")
    print(f"      ğŸ“Š ê²€ì¦ Accuracy: {fold_acc_lgb:.4f}")
    print(f"      ğŸ“Š Best Iteration: {lgb_model.best_iteration}")

# LightGBM OOF ì „ì²´ ì„±ëŠ¥ í‰ê°€
oof_pred_classes_lgb = np.argmax(oof_predictions_lgb, axis=1)
oof_f1_lgb = f1_score(y, oof_pred_classes_lgb, average='macro')
oof_acc_lgb = accuracy_score(y, oof_pred_classes_lgb)

print(f"\nğŸ‰ LightGBM OOF í•™ìŠµ ì™„ë£Œ!")
print(f"   ğŸ“Š í‰ê·  CV F1 Score: {np.mean(lgb_fold_scores):.4f} Â± {np.std(lgb_fold_scores):.4f}")
print(f"   ğŸ“Š OOF F1 Score: {oof_f1_lgb:.4f}")
print(f"   ğŸ“Š OOF Accuracy: {oof_acc_lgb:.4f}")
print(f"   ğŸ“Š í´ë“œë³„ F1 ì ìˆ˜: {[f'{score:.4f}' for score in lgb_fold_scores]}")

# CatBoost OOF í•™ìŠµ (ë‹¤ì–‘ì„± ì¦ê°€)
print("\nğŸ± CatBoost OOF í•™ìŠµ ì‹œì‘ (Macro F1 ìµœì í™”)...")

# CatBoostìš© OOF ì˜ˆì¸¡ê°’ ì €ì¥ ë°°ì—´
oof_predictions_cat = np.zeros((len(X), 21))
test_predictions_cat = np.zeros((len(test_df), 21))
cat_fold_scores = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\nğŸ“‹ CatBoost Fold {fold + 1}/{N_FOLDS} ì‹œì‘...")
    
    # í´ë“œë³„ ë°ì´í„° ë¶„í• 
    X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
    y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
    
    # CatBoost íŒŒë¼ë¯¸í„° ì„¤ì • (ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì¡°ì •)
    cat_params = {
        'loss_function': 'MultiClass',
        'eval_metric': 'TotalF1:average=Macro',  # Macro F1 ì§ì ‘ ëª¨ë‹ˆí„°ë§
        'depth': 6,                              # ê¹Šì´ ê°ì†Œ (9â†’6)
        'l2_leaf_reg': 10,                       # ì •ê·œí™” ì™„í™”
        'learning_rate': 0.1,                    # í•™ìŠµë¥  ì¦ê°€ (ë¹ ë¥¸ ìˆ˜ë ´)
        'iterations': 1000,                      # ë°˜ë³µ ìˆ˜ ëŒ€í­ ê°ì†Œ (5000â†’1000)
        'random_seed': 42,
        'od_type': 'Iter',                       # ì¡°ê¸° ì¢…ë£Œ íƒ€ì…
        'od_wait': 50,                           # ì¡°ê¸° ì¢…ë£Œ patience ê°ì†Œ (200â†’50)
        'verbose': 100,                          # ì§„í–‰ìƒí™© í‘œì‹œ (Falseâ†’100)
        'auto_class_weights': 'Balanced',        # í´ë˜ìŠ¤ ê· í˜•
        'bootstrap_type': 'Bayesian',            # ë² ì´ì§€ì•ˆ ë¶€íŠ¸ìŠ¤íŠ¸ë©
        'bagging_temperature': 1.0,
        'random_strength': 1.0
    }
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_pool = Pool(X_fold_train, y_fold_train)
    val_pool = Pool(X_fold_val, y_fold_val)
    
    # ëª¨ë¸ í•™ìŠµ
    cat_model = CatBoostClassifier(**cat_params)
    cat_model.fit(
        train_pool,
        eval_set=val_pool,
        use_best_model=True,
        plot=False
    )
    
    # OOF ì˜ˆì¸¡
    val_pred_cat = cat_model.predict_proba(X_fold_val)
    oof_predictions_cat[val_idx] = val_pred_cat
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ëˆ„ì 
    test_pred_cat = cat_model.predict_proba(X_test)
    test_predictions_cat += test_pred_cat / N_FOLDS
    
    # í´ë“œ ì„±ëŠ¥ í‰ê°€
    val_pred_classes_cat = np.argmax(val_pred_cat, axis=1)
    fold_f1_cat = f1_score(y_fold_val, val_pred_classes_cat, average='macro')
    fold_acc_cat = accuracy_score(y_fold_val, val_pred_classes_cat)
    cat_fold_scores.append(fold_f1_cat)
    
    print(f"   âœ… CatBoost Fold {fold + 1} ì™„ë£Œ!")
    print(f"      ğŸ“Š ê²€ì¦ F1 Score: {fold_f1_cat:.4f}")
    print(f"      ğŸ“Š ê²€ì¦ Accuracy: {fold_acc_cat:.4f}")
    print(f"      ğŸ“Š Best Iteration: {cat_model.get_best_iteration()}")

# CatBoost OOF ì „ì²´ ì„±ëŠ¥ í‰ê°€
oof_pred_classes_cat = np.argmax(oof_predictions_cat, axis=1)
oof_f1_cat = f1_score(y, oof_pred_classes_cat, average='macro')
oof_acc_cat = accuracy_score(y, oof_pred_classes_cat)

print(f"\nğŸ‰ CatBoost OOF í•™ìŠµ ì™„ë£Œ!")
print(f"   ğŸ“Š í‰ê·  CV F1 Score: {np.mean(cat_fold_scores):.4f} Â± {np.std(cat_fold_scores):.4f}")
print(f"   ğŸ“Š OOF F1 Score: {oof_f1_cat:.4f}")
print(f"   ğŸ“Š OOF Accuracy: {oof_acc_cat:.4f}")
print(f"   ğŸ“Š í´ë“œë³„ F1 ì ìˆ˜: {[f'{score:.4f}' for score in cat_fold_scores]}")

# 2ëª¨ë¸ ìŠ¤íƒœí‚¹ (LightGBM + CatBoost) - ë”¥ëŸ¬ë‹ ëª¨ë¸ ì œì™¸
print(f"\nğŸ”¥ 2ëª¨ë¸ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì‹œì‘ (íŠ¸ë¦¬ê³„ì—´ë§Œ)...")
print(f"   ğŸŒŸ LightGBM OOF F1: {oof_f1_lgb:.4f}")
print(f"   ğŸ± CatBoost OOF F1: {oof_f1_cat:.4f}")

# 2ëª¨ë¸ ìŠ¤íƒœí‚¹ ê°€ì¤‘ì¹˜ ìµœì í™”
def optimize_2model_stacking_weights(lgb_pred, cat_pred, true_labels):
    """2ëª¨ë¸ ìµœì  ìŠ¤íƒœí‚¹ ê°€ì¤‘ì¹˜ íƒìƒ‰"""
    best_score = 0
    best_weight = 0.5
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ (0.1 ê°„ê²©)
    for weight in np.arange(0.0, 1.1, 0.1):
        combined_pred = weight * lgb_pred + (1 - weight) * cat_pred
        combined_classes = np.argmax(combined_pred, axis=1)
        score = f1_score(true_labels, combined_classes, average='macro')
        
        if score > best_score:
            best_score = score
            best_weight = weight
    
    return best_weight, best_score

# ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰
optimal_weight, optimal_score = optimize_2model_stacking_weights(
    oof_predictions_lgb, oof_predictions_cat, y
)

print(f"   ğŸ¯ ìµœì  ê°€ì¤‘ì¹˜:")
print(f"      ğŸŒŸ LightGBM: {optimal_weight:.1f}")
print(f"      ğŸ± CatBoost: {1-optimal_weight:.1f}")
print(f"   ğŸ† ìŠ¤íƒœí‚¹ F1 Score: {optimal_score:.4f}")

# ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
individual_best = max(oof_f1_lgb, oof_f1_cat)
improvement = optimal_score - individual_best
print(f"   ğŸ“ˆ ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ í–¥ìƒ: +{improvement:.4f}")

# ìµœì¢… í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (2ëª¨ë¸ ìŠ¤íƒœí‚¹ ì ìš©)
final_test_predictions = (optimal_weight * test_predictions_lgb + 
                         (1 - optimal_weight) * test_predictions_cat)
final_test_classes = np.argmax(final_test_predictions, axis=1)

print("=" * 60)

# ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±
print("\nğŸ“ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")

# ì œì¶œ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸
submission_df['target'] = final_test_classes

# ê²°ê³¼ ì €ì¥
output_path = 'C:/Users/jsy/Desktop/coretech/Dacon/smart/data/stacking_ensemble_submission.csv'
submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

# ìµœì¢… ê²°ê³¼ ìš”ì•½
print(f"\nğŸ“‹ ìµœì¢… ì„±ëŠ¥ ìš”ì•½ (íŠ¸ë¦¬ê³„ì—´ ëª¨ë¸ë§Œ):")
print(f"   ğŸŒŸ LightGBM OOF F1: {oof_f1_lgb:.4f}")
print(f"   ğŸ± CatBoost OOF F1: {oof_f1_cat:.4f}")
print(f"   ğŸ”¥ 2ëª¨ë¸ ìŠ¤íƒœí‚¹ F1: {optimal_score:.4f}")
print(f"   ğŸ¯ ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ í–¥ìƒ: +{improvement:.4f}")
print(f"   ğŸ“Š íŠ¸ë¦¬ê³„ì—´ ëª¨ë¸ ê°•í™” íš¨ê³¼:")
print(f"      âœ… Macro F1 ì»¤ìŠ¤í…€ í‰ê°€ ì ìš©")
print(f"      âœ… íŠ¹ì§• ê³µí•™ (í†µê³„ + PCA): {X.shape[1]}ê°œ íŠ¹ì§•")
print(f"      âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (num_leavesâ†‘, ì •ê·œí™”â†‘)")
print(f"      âœ… CatBoost ì¶”ê°€ë¡œ ë‹¤ì–‘ì„± í™•ë³´")
print(f"      ğŸš€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì œì™¸ë¡œ ë¹ ë¥¸ ì‹¤í–‰ ê°€ëŠ¥")

# ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ í™•ì¸ 
print(f"\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:")
unique, counts = np.unique(final_test_classes, return_counts=True)
for cls, count in zip(unique, counts):
    print(f"   í´ë˜ìŠ¤ {cls}: {count}ê°œ")

print("\n" + "=" * 60)
print("ğŸ‰ ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ!")
print("ì£¼ìš” ê°œì„ ì‚¬í•­:")
print("âœ… íŠ¹ì§• ê³µí•™: í†µê³„ì  íŠ¹ì§• + PCA")
print("âœ… LightGBM Macro F1 íŠœë‹ + ê°•í™”ëœ íŒŒë¼ë¯¸í„°")
print("âœ… CatBoost ì¶”ê°€ë¡œ ë‹¤ì–‘ì„± í™•ë³´")
print("âœ… 2ëª¨ë¸ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (LGB + CAT)")
print("âœ… StratifiedKFold OOF êµì°¨ê²€ì¦")
print("ğŸš€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì œì™¸ë¡œ ë¹ ë¥¸ ì‹¤í–‰")
print("=" * 60)