"""
LightGBM ê¸°ë°˜ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ (F1 ìµœì í™” ë²„ì „)
- IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ìœ¼ë¡œ ì™„í™” ì²˜ë¦¬
- ì „ì²˜ë¦¬ ë‹¨ìˆœí™”: ìŠ¤ì¼€ì¼ë§/ì •ê·œí™” ì œê±° (íŠ¸ë¦¬ ëª¨ë¸ íŠ¹ì„±)
- Macro F1 ì§ì ‘ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
- ì‚¬ìš©ì ì •ì˜ F1 í‰ê°€ í•¨ìˆ˜ë¡œ early stopping
- ì¥ê¸° í•™ìŠµ & ë†’ì€ ê·œì œë¡œ ê³¼ì í•© ë°©ì§€
- Stratified Splitìœ¼ë¡œ ë¼ë²¨ ê· í˜• ìœ ì§€
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# ì‹œë“œ ê³ ì •
np.random.seed(42)

print("=" * 60)
print("LightGBM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ë¹„ì •ìƒ ì‘ë™ ë¶„ë¥˜ ëª¨ë¸ (F1 ìµœì í™”)")
print("=" * 60)

# 1. ë°ì´í„° ì ì¬ ë° ë¶„ë¦¬
print("\n1. ë°ì´í„° ì ì¬ ì¤‘...")
train_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/train.csv')
test_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/test.csv')
submission_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/sample_submission.csv')

# X, y ë¶„ë¦¬
X = train_df.drop(columns=['target', 'ID'])
y = train_df['target']  # ì •ìˆ˜ ë¼ë²¨ ê·¸ëŒ€ë¡œ ìœ ì§€ (0~20)

print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"í”¼ì²˜ ìˆ˜: {X.shape[1]}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {len(y.unique())}")
print(f"í´ë˜ìŠ¤ ë¶„í¬:\n{y.value_counts().sort_index()}")

# 2. ì‚¬ìš©ì ì •ì˜ Macro F1 í‰ê°€ í•¨ìˆ˜ ì •ì˜
print("\n2. ì‚¬ìš©ì ì •ì˜ Macro F1 í‰ê°€ í•¨ìˆ˜ ì •ì˜...")

def lgb_macro_f1_eval(y_pred, y_true):
    """LightGBMìš© Macro F1 í‰ê°€ í•¨ìˆ˜"""
    y_true = y_true.get_label()
    y_pred = y_pred.reshape(21, -1).T
    y_pred_classes = np.argmax(y_pred, axis=1)
    f1 = f1_score(y_true, y_pred_classes, average='macro')
    return 'macro_f1', f1, True  # (eval_name, eval_result, is_higher_better)

print("ì‚¬ìš©ì ì •ì˜ Macro F1 í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# 3. ë°ì´í„° ë¶„í•  (Stratified Splitìœ¼ë¡œ ë¼ë²¨ ê· í˜• ìœ ì§€)
print("\n3. ë°ì´í„° ë¶„í•  ì¤‘...")
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]}ê°œ")
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape[0]}ê°œ")

# 4. ì´ìƒì¹˜ í´ë¦¬í•‘ (ì™„í™” ì²˜ë¦¬)
print("\n4. IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ ì ìš© ì¤‘...")

def apply_iqr_clipping(X_train, X_val, X_test):
    """IQR ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ í´ë¦¬í•‘í•˜ëŠ” í•¨ìˆ˜"""
    X_train_clipped = X_train.copy()
    X_val_clipped = X_val.copy()
    X_test_clipped = X_test.copy()
    
    clip_info = {}
    
    for column in X_train.columns:
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ IQR ê³„ì‚°
        Q1 = X_train[column].quantile(0.25)
        Q3 = X_train[column].quantile(0.75)
        IQR = Q3 - Q1
        
        # ê²½ê³„ê°’ ì„¤ì •
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        clip_info[column] = (lower_bound, upper_bound)
        
        # ëª¨ë“  ë°ì´í„°ì…‹ì— í´ë¦¬í•‘ ì ìš©
        X_train_clipped[column] = X_train[column].clip(lower_bound, upper_bound)
        X_val_clipped[column] = X_val[column].clip(lower_bound, upper_bound)
        X_test_clipped[column] = X_test[column].clip(lower_bound, upper_bound)
    
    return X_train_clipped, X_val_clipped, X_test_clipped, clip_info

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
X_test = test_df.drop(columns=['ID'])

# í´ë¦¬í•‘ ì ìš©
X_train_clipped, X_val_clipped, X_test_clipped, clip_info = apply_iqr_clipping(
    X_train, X_val, X_test
)

print(f"í´ë¦¬í•‘ ì™„ë£Œ. ì²˜ë¦¬ëœ í”¼ì²˜ ìˆ˜: {len(clip_info)}")

# ìµœì¢… ë°ì´í„° (ì´ìƒì¹˜ ì²˜ë¦¬ë¨, ìŠ¤ì¼€ì¼ë§ì€ ì œê±°)
X_train_final = X_train_clipped
X_test_final = X_test_clipped

print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„°: {X_train_final.shape}")
print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_final.shape}")
print("ì „ì²˜ë¦¬: IQR ì´ìƒì¹˜ í´ë¦¬í•‘ë§Œ ì ìš© (ìŠ¤ì¼€ì¼ë§/ì •ê·œí™”ëŠ” íŠ¸ë¦¬ ëª¨ë¸ íŠ¹ì„±ìƒ ì œê±°)")

# 4. LightGBM ëª¨ë¸ êµ¬ì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (F1 ìµœì í™”)
print("\n4. LightGBM ëª¨ë¸ êµ¬ì„± ì¤‘ (F1 ìµœì í™” ì„¤ì •)...")

# LightGBM F1 ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì¶©ë¶„í•œ í›ˆë ¨)
lgb_params = {
    'objective': 'multiclass',
    'num_class': 21,
    'metric': 'None',  # ì‚¬ìš©ì ì •ì˜ í‰ê°€ í•¨ìˆ˜ ì‚¬ìš©
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.03,  # ëŸ¬ë‹ë ˆì´íŠ¸ ë” ê°ì†Œ (0.05 â†’ 0.03) - ë” ì„¸ë°€í•œ í•™ìŠµ
    'feature_fraction': 0.8,  # ê·œì œ ê°•í™” (0.9 â†’ 0.8)
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'lambda_l1': 0.05,  # L1 ê·œì œ ì™„í™” (0.1 â†’ 0.05) - ë” ê¸´ í›ˆë ¨ í—ˆìš©
    'lambda_l2': 0.05,  # L2 ê·œì œ ì™„í™” (0.1 â†’ 0.05) - ë” ê¸´ í›ˆë ¨ í—ˆìš©
    'min_data_in_leaf': 15,  # ê³¼ì í•© ë°©ì§€ ì™„í™” (20 â†’ 15)
    'verbose': -1,
    'random_state': 42,
    'n_jobs': -1
}

print("LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
for key, value in lgb_params.items():
    print(f"  {key}: {value}")

# ì…ë ¥ ì°¨ì› í™•ì¸
input_dim = X_train_final.shape[1]
print(f"\nì…ë ¥ ì°¨ì›: {input_dim}")
print(f"í´ë˜ìŠ¤ ìˆ˜: 21")
print("F1 ìµœì í™” ì„¤ì •: ëŸ¬ë‹ë ˆì´íŠ¸ 0.03, ìµœëŒ€ 5000 ë¼ìš´ë“œ, 500 ë¼ìš´ë“œ ì¸ë‚´ì‹¬")

# 5. êµì°¨ ê²€ì¦ ì„¤ì •
print("\n5. Stratified K-Fold êµì°¨ ê²€ì¦ ì„¤ì • ì¤‘...")

# Stratified K-Fold ì„¤ì • (í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤)
n_folds = 5
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

print(f"êµì°¨ ê²€ì¦: {n_folds}-Fold Stratified K-Fold")
print("ê° í´ë“œë§ˆë‹¤ í´ë˜ìŠ¤ ë¶„í¬ê°€ ê· ë“±í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.")

# 6. LightGBM ëª¨ë¸ í•™ìŠµ ë° êµì°¨ ê²€ì¦ (F1 ìµœì í™”)
print("\n6. LightGBM ëª¨ë¸ êµì°¨ ê²€ì¦ í•™ìŠµ ì‹œì‘ (F1 ìµœì í™”)...")

print(f"\nğŸš€ LightGBM F1 ìµœì í™” ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
print(f"   ğŸ“ˆ êµì°¨ ê²€ì¦: {n_folds}-Fold")
print(f"   ğŸ¯ ëª©í‘œ: Macro F1 ìŠ¤ì½”ì–´ ì§ì ‘ ìµœì í™”")
print(f"   ğŸ”§ ëª¨ë¸: LightGBM (ì‚¬ìš©ì ì •ì˜ F1 í‰ê°€)")
print(f"   ğŸŒŸ íŠ¹ì§•: IQR ì´ìƒì¹˜ í´ë¦¬í•‘ + F1 ê¸°ì¤€ early stopping")
print(f"   âš™ï¸ ì„¤ì •: í•™ìŠµë¥  0.03 + ìµœëŒ€ 5000 ë¼ìš´ë“œ + 500 ì¸ë‚´ì‹¬")
print(f"   ğŸƒ í›ˆë ¨: ì¶©ë¶„í•œ í›ˆë ¨ì„ ìœ„í•œ ê¸´ í•™ìŠµ ì„¤ì •")
print("=" * 60)

# êµì°¨ ê²€ì¦ ê²°ê³¼ ì €ì¥ìš©
cv_scores = []
cv_f1_scores = []
oof_predictions = np.zeros(len(X_train_final))  # Out-of-fold ì˜ˆì¸¡ê°’
test_predictions = np.zeros((len(X_test_final), 21))  # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ (í™•ë¥ )

fold_num = 1
for train_idx, val_idx in skf.split(X_train_final, y_train):
    print(f"\nğŸ“Š Fold {fold_num}/{n_folds} í•™ìŠµ ì¤‘...")
    
    # ë°ì´í„° ë¶„í• 
    X_fold_train, X_fold_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]
    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    
    # LightGBM ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_fold_train, label=y_fold_train)
    val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)
    
    # ëª¨ë¸ í•™ìŠµ (F1 ìµœì í™” - ì¶©ë¶„í•œ í›ˆë ¨)
    model = lgb.train(
        lgb_params,
        train_data,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        num_boost_round=5000,  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ëŒ€í­ ì¦ê°€ (2000 â†’ 5000)
        feval=lgb_macro_f1_eval,  # ì‚¬ìš©ì ì •ì˜ F1 í‰ê°€ í•¨ìˆ˜
        callbacks=[
            lgb.early_stopping(stopping_rounds=500, verbose=True),  # ë” ê¸´ ì¸ë‚´ì‹¬ (200 â†’ 500)
            lgb.log_evaluation(period=100)  # 100 ë¼ìš´ë“œë§ˆë‹¤ ì„±ëŠ¥ ì¶œë ¥
        ]
    )
    
    # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
    val_pred_proba = model.predict(X_fold_val, num_iteration=model.best_iteration)
    val_pred_classes = np.argmax(val_pred_proba, axis=1)
    
    # Out-of-fold ì˜ˆì¸¡ê°’ ì €ì¥
    oof_predictions[val_idx] = val_pred_classes
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (í‰ê· ì„ ìœ„í•´ ëˆ„ì )
    test_pred_proba = model.predict(X_test_final, num_iteration=model.best_iteration)
    test_predictions += test_pred_proba / n_folds
    
    # ì„±ëŠ¥ í‰ê°€
    fold_accuracy = accuracy_score(y_fold_val, val_pred_classes)
    fold_f1 = f1_score(y_fold_val, val_pred_classes, average='macro')
    
    cv_scores.append(fold_accuracy)
    cv_f1_scores.append(fold_f1)
    
    print(f"   âœ… Fold {fold_num} ì™„ë£Œ:")
    print(f"      ğŸ“ˆ ì •í™•ë„: {fold_accuracy:.4f}")
    print(f"      ğŸ¯ F1 ì ìˆ˜: {fold_f1:.4f}")
    print(f"      ğŸŒŸ ìµœì  ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ: {model.best_iteration}")
    
    fold_num += 1

# ì „ì²´ êµì°¨ ê²€ì¦ ê²°ê³¼
print(f"\nğŸ‰ êµì°¨ ê²€ì¦ ì™„ë£Œ!")
print(f"   ğŸ“Š í‰ê·  ì •í™•ë„: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
print(f"   ğŸ¯ í‰ê·  F1 ì ìˆ˜: {np.mean(cv_f1_scores):.4f} Â± {np.std(cv_f1_scores):.4f}")
print(f"   ğŸ† ìµœê³  ì •í™•ë„: {max(cv_scores):.4f}")
print(f"   ğŸ’« ìµœê³  F1 ì ìˆ˜: {max(cv_f1_scores):.4f}")
print("=" * 60)

# 7. Out-of-Fold ì˜ˆì¸¡ í‰ê°€ (F1 ìµœì í™” ê²°ê³¼)
print("\n7. Out-of-Fold ì˜ˆì¸¡ í‰ê°€ ì¤‘ (F1 ìµœì í™” ê²°ê³¼)...")

# Out-of-fold ì˜ˆì¸¡ í‰ê°€ (ì „ì²´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ êµì°¨ ê²€ì¦ ì˜ˆì¸¡)
oof_accuracy = accuracy_score(y_train, oof_predictions)
oof_f1 = f1_score(y_train, oof_predictions, average='macro')

print(f"Out-of-Fold ì •í™•ë„: {oof_accuracy:.4f}")
print(f"Out-of-Fold Macro F1 ì ìˆ˜: {oof_f1:.4f}")

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\nOut-of-Fold ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_train, oof_predictions))

# êµì°¨ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 5))

# í´ë“œë³„ ì„±ëŠ¥ ì‹œê°í™”
plt.subplot(1, 3, 1)
folds = range(1, n_folds + 1)
plt.bar(folds, cv_scores, alpha=0.7, label='Accuracy', color='skyblue')
plt.axhline(np.mean(cv_scores), color='red', linestyle='--', label=f'Mean: {np.mean(cv_scores):.4f}')
plt.title('Cross-Validation Accuracy')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.legend()

plt.subplot(1, 3, 2)
plt.bar(folds, cv_f1_scores, alpha=0.7, label='F1 Score', color='lightgreen')
plt.axhline(np.mean(cv_f1_scores), color='red', linestyle='--', label=f'Mean: {np.mean(cv_f1_scores):.4f}')
plt.title('Cross-Validation F1 Score')
plt.xlabel('Fold')
plt.ylabel('F1 Score')
plt.ylim(0, 1)
plt.legend()

# Out-of-Fold í˜¼ë™ í–‰ë ¬
plt.subplot(1, 3, 3)
cm = confusion_matrix(y_train, oof_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Out-of-Fold Confusion Matrix (F1 Optimized)')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.savefig('C:/Users/jsy/Desktop/coretech/Dacon/smart/model/training_results.png', dpi=300, bbox_inches='tight')
plt.show()

# 8. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (F1 ìµœì í™” ì•™ìƒë¸”)
print("\n8. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘ (F1 ìµœì í™” ì•™ìƒë¸”)...")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (êµì°¨ ê²€ì¦ìœ¼ë¡œ ì´ë¯¸ ê³„ì‚°ë¨)
test_pred_classes = np.argmax(test_predictions, axis=1)

print(f"í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ: {len(test_pred_classes)}ê°œ ìƒ˜í”Œ")
print("F1 ìµœì í™” êµì°¨ ê²€ì¦ ì•™ìƒë¸” ì˜ˆì¸¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# 9. ì œì¶œ íŒŒì¼ ìƒì„±
print("\n9. ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")

submission_df['target'] = test_pred_classes

# ê²°ê³¼ ì €ì¥
output_path = 'C:/Users/jsy/Desktop/coretech/Dacon/smart/data/f1_optimized_submission.csv'
submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"F1 ìµœì í™” ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

# ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ í™•ì¸
print(f"\nì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:")
unique, counts = np.unique(test_pred_classes, return_counts=True)
for cls, count in zip(unique, counts):
    print(f"í´ë˜ìŠ¤ {cls}: {count}ê°œ")

print("\n" + "=" * 60)
print("LightGBM F1 ìµœì í™” ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ!")
print("ì£¼ìš” ê°œì„ ì‚¬í•­:")
print("- IQR ì´ìƒì¹˜ í´ë¦¬í•‘ ì ìš© (ìŠ¤ì¼€ì¼ë§/ì •ê·œí™”ëŠ” ì œê±°)")
print("- Macro F1 ì§ì ‘ ìµœì í™” (ì‚¬ìš©ì ì •ì˜ í‰ê°€ í•¨ìˆ˜)")
print("- F1 ê¸°ì¤€ early stopping")
print("- ì¶©ë¶„í•œ í›ˆë ¨: í•™ìŠµë¥  0.03 + ìµœëŒ€ 5000 ë¼ìš´ë“œ + 500 ì¸ë‚´ì‹¬")
print("=" * 60)