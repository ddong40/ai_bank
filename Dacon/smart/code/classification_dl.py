"""
Conv1D + Transformer ê¸°ë°˜ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸
- IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ìœ¼ë¡œ ì™„í™” ì²˜ë¦¬
- Stratified Splitìœ¼ë¡œ ë¼ë²¨ ê· í˜• ìœ ì§€
- Conv1D(2ì¸µ) + Sinusoidal Positional Encoding + Multi-Head Attention Transformerë¡œ 21ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜
- Transformer ë…¼ë¬¸ê³¼ ë™ì¼í•œ cos/sin ê¸°ë°˜ í¬ì§€ì…”ë„ ì¸ì½”ë”© ì ìš©
- EarlyStopping ë° ReduceLROnPlateau ì ìš©
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ë”¥ëŸ¬ë‹ ë° ì „ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, BatchNormalization, Dropout, GlobalAveragePooling1D,
    Conv1D, MaxPooling1D, MultiHeadAttention, LayerNormalization,
    Reshape, Concatenate, Add
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.losses import SparseCategoricalCrossentropy

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns

# ì‹œë“œ ê³ ì •
tf.random.set_seed(42)
np.random.seed(42)

# GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš© (GPU ì‚¬ìš© ì‹œ)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

print("=" * 60)
print("Conv1D + Transformer ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ë¹„ì •ìƒ ì‘ë™ ë¶„ë¥˜ ëª¨ë¸")
print("=" * 60)

# 1. ë°ì´í„° ì ì¬ ë° ë¶„ë¦¬
print("\n1. ë°ì´í„° ì ì¬ ì¤‘...")
train_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/train.csv')
test_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/test.csv')
submission_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/sample_submission.csv')

# X, y ë¶„ë¦¬
X = train_df.drop(columns=['target', 'ID'])
y = train_df['target']  # ì •ìˆ˜ ë¼ë²¨ ê·¸ëŒ€ë¡œ ìœ ì§€ (0~20)

print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"í”¼ì²˜ ìˆ˜: {X.shape[1]}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {len(y.unique())}")
print(f"í´ë˜ìŠ¤ ë¶„í¬:\n{y.value_counts().sort_index()}")

# 2. Stratified Split (ë¼ë²¨ ê· í˜• ìœ ì§€)
print("\n2. ë°ì´í„° ë¶„í•  ì¤‘...")
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]}ê°œ")
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape[0]}ê°œ")

# í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ class_weight ê³„ì‚° (Macro F1 Score ìµœì í™”)
print("\nğŸ“Š í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°...")
class_weights = compute_class_weight(
    'balanced', 
    classes=np.unique(y_train), 
    y=y_train
)
class_weight_dict = dict(zip(np.unique(y_train), class_weights))

print("í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:")
for cls, weight in class_weight_dict.items():
    count = (y_train == cls).sum()
    print(f"  í´ë˜ìŠ¤ {cls}: ê°€ì¤‘ì¹˜ {weight:.3f} (ìƒ˜í”Œ ìˆ˜: {count})")

print(f"\nê°€ì¤‘ì¹˜ ë²”ìœ„: {min(class_weights):.3f} ~ {max(class_weights):.3f}")
print("â†’ Macro F1 Score ìµœì í™”ë¥¼ ìœ„í•´ ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬")

# 3. ì´ìƒì¹˜ í´ë¦¬í•‘ (ì™„í™” ì²˜ë¦¬)
print("\n3. IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ ì ìš© ì¤‘...")

def apply_iqr_clipping(X_train, X_val, X_test, test_clipping=True, test_multiplier=3.0):
    """
    IQR ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ í´ë¦¬í•‘í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        test_clipping: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— í´ë¦¬í•‘ ì ìš© ì—¬ë¶€
        test_multiplier: í…ŒìŠ¤íŠ¸ ë°ì´í„°ìš© IQR ë°°ìˆ˜ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
    """
    X_train_clipped = X_train.copy()
    X_val_clipped = X_val.copy()
    X_test_clipped = X_test.copy()
    
    clip_info = {}
    
    for column in X_train.columns:
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ IQR ê³„ì‚°
        Q1 = X_train[column].quantile(0.25)
        Q3 = X_train[column].quantile(0.75)
        IQR = Q3 - Q1
        
        # í›ˆë ¨/ê²€ì¦ìš© ê²½ê³„ê°’ (ì¼ë°˜ì ì¸ 1.5 * IQR)
        train_lower = Q1 - 1.5 * IQR
        train_upper = Q3 + 1.5 * IQR
        
        # í…ŒìŠ¤íŠ¸ìš© ê²½ê³„ê°’ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        test_lower = Q1 - test_multiplier * IQR
        test_upper = Q3 + test_multiplier * IQR
        
        clip_info[column] = {
            'train_bounds': (train_lower, train_upper),
            'test_bounds': (test_lower, test_upper)
        }
        
        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° í´ë¦¬í•‘ (ê¸°ì¡´ê³¼ ë™ì¼)
        X_train_clipped[column] = X_train[column].clip(train_lower, train_upper)
        X_val_clipped[column] = X_val[column].clip(train_lower, train_upper)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë¦¬í•‘ (ì„ íƒì  ì ìš©)
        if test_clipping:
            X_test_clipped[column] = X_test[column].clip(test_lower, test_upper)
            
    return X_train_clipped, X_val_clipped, X_test_clipped, clip_info

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
X_test = test_df.drop(columns=['ID'])

# í´ë¦¬í•‘ ì ìš© (í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë” ê´€ëŒ€í•œ ê¸°ì¤€ ì ìš©)
X_train_clipped, X_val_clipped, X_test_clipped, clip_info = apply_iqr_clipping(
    X_train, X_val, X_test, 
    test_clipping=True,      # í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë¦¬í•‘ ì ìš©
    test_multiplier=3.0      # 1.5 ëŒ€ì‹  3.0 * IQR (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
)

print(f"í´ë¦¬í•‘ ì™„ë£Œ. ì²˜ë¦¬ëœ í”¼ì²˜ ìˆ˜: {len(clip_info)}")
print(f"ğŸ“‹ í´ë¦¬í•‘ ì„¤ì •:")
print(f"   ğŸ”¸ í›ˆë ¨/ê²€ì¦ ë°ì´í„°: 1.5 * IQR ê¸°ì¤€ (ì—„ê²©)")
print(f"   ğŸ”¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°: 3.0 * IQR ê¸°ì¤€ (ê´€ëŒ€) - ì •ë³´ ì†ì‹¤ ìµœì†Œí™”")

# í´ë¦¬í•‘ëœ ë°ì´í„° ë¹„êµ
train_outliers = (X_train != X_train_clipped).sum().sum()
test_outliers = (X_test != X_test_clipped).sum().sum()
print(f"   ğŸ“Š í´ë¦¬í•‘ëœ ê°’ ìˆ˜ - í›ˆë ¨: {train_outliers}ê°œ, í…ŒìŠ¤íŠ¸: {test_outliers}ê°œ")

# 4. ì •ê·œí™” (MinMax Scaling)
print("\n4. MinMaxScalerë¥¼ ì´ìš©í•œ ì •ê·œí™” ì¤‘...")
scaler = MinMaxScaler()

# í›ˆë ¨ ë°ì´í„°ì— fit, ëª¨ë“  ë°ì´í„°ì— transform (0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§)
X_train_scaled = scaler.fit_transform(X_train_clipped)
X_val_scaled = scaler.transform(X_val_clipped)
X_test_scaled = scaler.transform(X_test_clipped)

print("MinMax ì •ê·œí™” ì™„ë£Œ (ë²”ìœ„: 0-1)")

# ì •ê·œí™” ìƒíƒœ ê²€ì¦
print(f"\nğŸ“‹ ì •ê·œí™” ìƒíƒœ ê²€ì¦:")
print(f"   ğŸ”¸ í›ˆë ¨ ë°ì´í„° ë²”ìœ„: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]")
print(f"   ğŸ”¸ ê²€ì¦ ë°ì´í„° ë²”ìœ„: [{X_val_scaled.min():.3f}, {X_val_scaled.max():.3f}]")
print(f"   ğŸ”¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²”ìœ„: [{X_test_scaled.min():.3f}, {X_test_scaled.max():.3f}]")
print(f"   âœ… ëª¨ë“  ë°ì´í„°ê°€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ì •ê·œí™”ë¨")

# 5. ëª¨ë¸ êµ¬ì„± (Conv1D + Positional Encoding + Transformer)
print("\n5. Conv1D + Positional Encoding + Transformer ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì„± ì¤‘...")

class PositionalEncoding(tf.keras.layers.Layer):
    """
    Transformer ë…¼ë¬¸ì˜ Sinusoidal Positional Encodingì„ Keras Layerë¡œ êµ¬í˜„
    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    
    def __init__(self, max_len=1000, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.max_len = max_len
        
    def get_angles(self, pos, i, d_model):
        """ê°ë„ ê³„ì‚° í•¨ìˆ˜"""
        angle_rates = 1 / tf.pow(10000.0, tf.cast(2 * (i // 2), tf.float32) / tf.cast(d_model, tf.float32))
        return pos * angle_rates
    
    def call(self, inputs):
        seq_len = tf.shape(inputs)[1]
        d_model = tf.shape(inputs)[2]
        
        # ìœ„ì¹˜ ì¸ë±ìŠ¤ ìƒì„± (0, 1, 2, ..., seq_len-1)
        pos = tf.cast(tf.range(seq_len), tf.float32)[:, tf.newaxis]
        
        # ì°¨ì› ì¸ë±ìŠ¤ ìƒì„± (0, 1, 2, ..., d_model-1)
        i = tf.cast(tf.range(d_model), tf.float32)[tf.newaxis, :]
        
        # ê°ë„ ê³„ì‚°
        angle_rads = self.get_angles(pos, i, d_model)
        
        # ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin, í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cos ì ìš©
        sines = tf.sin(angle_rads[:, 0::2])
        cosines = tf.cos(angle_rads[:, 1::2])
        
        # sinê³¼ cosë¥¼ ë²ˆê°ˆì•„ê°€ë©° ë°°ì¹˜
        pos_encoding = tf.concat([sines, cosines], axis=-1)
        
        # d_modelì´ í™€ìˆ˜ì¸ ê²½ìš° ë§ˆì§€ë§‰ ì°¨ì› ì¡°ì •
        if d_model % 2 == 1:
            pos_encoding = pos_encoding[:, :-1]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        pos_encoding = pos_encoding[tf.newaxis, :, :]
        
        # ì…ë ¥ê³¼ positional encoding í•©ì‚°
        return inputs + pos_encoding
    
    def get_config(self):
        config = super().get_config()
        config.update({"max_len": self.max_len})
        return config

def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    """Transformer ë¸”ë¡ êµ¬ì„± í•¨ìˆ˜"""
    # Multi-Head Attention
    attention_output = MultiHeadAttention(
        num_heads=num_heads, 
        key_dim=head_size,
        dropout=dropout
    )(inputs, inputs)
    
    # Add & Norm 1
    attention_output = Dropout(dropout)(attention_output)
    x1 = Add()([inputs, attention_output])
    x1 = LayerNormalization(epsilon=1e-6)(x1)
    
    # Feed Forward Network
    ffn_output = Dense(ff_dim, activation='relu')(x1)
    ffn_output = Dropout(dropout)(ffn_output)
    ffn_output = Dense(inputs.shape[-1])(ffn_output)
    
    # Add & Norm 2
    ffn_output = Dropout(dropout)(ffn_output)
    x2 = Add()([x1, ffn_output])
    x2 = LayerNormalization(epsilon=1e-6)(x2)
    
    return x2

def create_dual_branch_model(input_dim, num_classes=21):
    """ë“€ì–¼ ë¸Œëœì¹˜ Dilated Conv1D + Transformer ëª¨ë¸ (x, z ë¸Œëœì¹˜)"""
    
    # ì…ë ¥ì¸µ (ë™ì¼í•œ ë°ì´í„°ê°€ ë‘ ë¸Œëœì¹˜ë¡œ ë¶„ê¸°)
    inputs = Input(shape=(input_dim,))
    
    # 1D ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ (52 í”¼ì²˜ â†’ 52 ì‹œê°„ìŠ¤í…)
    reshaped = Reshape((input_dim, 1))(inputs)
    
    # ===== X ë¸Œëœì¹˜: Dilated Conv1Dë¡œ ì„¸ë°€í•œ ì‹œê°„ì  íŒ¨í„´ ì¶”ì¶œ =====
    # ì‘ì€ dilation rateë¡œ ë¡œì»¬ íŒ¨í„´ì— ì§‘ì¤‘
    x = Conv1D(filters=64, kernel_size=3, dilation_rate=1, padding='same', name='x_dilconv1')(reshaped)
    x = tf.keras.layers.ReLU(name='x_relu1')(x)
    x = LayerNormalization(epsilon=1e-6, name='x_ln1')(x)
    x = Dropout(0.1, name='x_dropout1')(x)
    
    x = Conv1D(filters=32, kernel_size=3, dilation_rate=2, padding='same', name='x_dilconv2')(x)
    x = tf.keras.layers.ReLU(name='x_relu2')(x)
    x = LayerNormalization(epsilon=1e-6, name='x_ln2')(x)
    x = MaxPooling1D(pool_size=2, padding='same', name='x_pool')(x)  # 26 timesteps
    x = Dropout(0.1, name='x_dropout2')(x)
    
    # ===== Z ë¸Œëœì¹˜: Dilated Conv1Dë¡œ ì¥ê±°ë¦¬ ì‹œê°„ì  ì˜ì¡´ì„± í¬ì°© =====
    # í° dilation rateë¡œ ê¸€ë¡œë²Œ íŒ¨í„´ì— ì§‘ì¤‘
    z = Conv1D(filters=32, kernel_size=3, dilation_rate=4, padding='same', name='z_dilconv1')(reshaped)
    z = tf.keras.layers.ReLU(name='z_relu1')(z)
    z = LayerNormalization(epsilon=1e-6, name='z_ln1')(z)
    z = Dropout(0.1, name='z_dropout1')(z)
    
    z = Conv1D(filters=16, kernel_size=3, dilation_rate=8, padding='same', name='z_dilconv2')(z)
    z = tf.keras.layers.ReLU(name='z_relu2')(z)
    z = LayerNormalization(epsilon=1e-6, name='z_ln2')(z)
    z = MaxPooling1D(pool_size=2, padding='same', name='z_pool')(z)  # 26 timesteps
    z = Dropout(0.1, name='z_dropout2')(z)
    
    # ===== ë¸Œëœì¹˜ ê²°í•© =====
    # x: (batch, 26, 32), z: (batch, 26, 16) â†’ concat â†’ (batch, 26, 48)
    combined = Concatenate(axis=-1, name='branch_concat')([x, z])
    
    print(f"ë¸Œëœì¹˜ ê²°í•© í›„ shape: {combined.shape}")
    
    # ===== Positional Encoding ì¶”ê°€ =====
    # Transformer ë…¼ë¬¸ê³¼ ë™ì¼í•œ sin/cos ê¸°ë°˜ í¬ì§€ì…”ë„ ì¸ì½”ë”© ì ìš©
    pos_encoder = PositionalEncoding(name='positional_encoding')
    combined_with_pos = pos_encoder(combined)
    
    # ===== Transformer ë¸”ë¡ (ê²°í•©ëœ íŠ¹ì§•ìœ¼ë¡œ ê¸€ë¡œë²Œ ê´€ê³„ í•™ìŠµ) =====
    transformer_out = transformer_block(
        inputs=combined_with_pos,  # í¬ì§€ì…”ë„ ì¸ì½”ë”©ì´ ì¶”ê°€ëœ ì…ë ¥ ì‚¬ìš©
        head_size=16,      # 48 ì±„ë„ì— ë§ê²Œ ì¡°ì •
        num_heads=3,       # 3ê°œ í—¤ë“œ ìœ ì§€
        ff_dim=96,         # 48 * 2
        dropout=0.1
    )
    
    # ===== Flatten for Direct Dense Connection =====
    # Transformer ì¶œë ¥ì„ ì§ì ‘ Denseì¸µìœ¼ë¡œ ì—°ê²° (GAP/MaxPool ì œê±°)
    flattened = tf.keras.layers.Flatten(name='flatten')(transformer_out)
    
    # ===== Classification Head =====
    clf = Dense(128, activation='relu', name='clf_dense1')(flattened)
    clf = LayerNormalization(epsilon=1e-6, name='clf_ln1')(clf)
    clf = Dropout(0.2, name='clf_dropout1')(clf)
    
    clf = Dense(64, activation='relu', name='clf_dense2')(clf)
    clf = LayerNormalization(epsilon=1e-6, name='clf_ln2')(clf)
    clf = Dropout(0.2, name='clf_dropout2')(clf)
    
    # ì¶œë ¥ì¸µ
    outputs = Dense(num_classes, activation='softmax', name='output')(clf)
    
    model = Model(inputs=inputs, outputs=outputs, name='DualBranchConvTransformer')
    return model

def create_ensemble_compatible_model(input_dim, num_classes=21, model_id=0):
    """ì•™ìƒë¸”ìš© ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ëª¨ë¸"""
    inputs = Input(shape=(input_dim,))
    
    if model_id == 0:  # Dilated Conv1D ê¸°ë³¸ ëª¨ë¸
        x = Reshape((input_dim, 1))(inputs)
        x = Conv1D(32, 3, dilation_rate=1, padding='same', activation='relu')(x)
        x = LayerNormalization()(x)
        x = Conv1D(64, 3, dilation_rate=3, padding='same', activation='relu')(x)
        x = LayerNormalization()(x)
        x = MaxPooling1D(2)(x)
        x = PositionalEncoding()(x)  # Positional Encoding ì¶”ê°€
        x = transformer_block(x, 16, 2, 128, 0.1)
        x = tf.keras.layers.Flatten()(x)  # GAP ëŒ€ì‹  Flatten ì‚¬ìš©
        
    elif model_id == 1:  # ë” ê¹Šì€ Dilated Conv1D
        x = Reshape((input_dim, 1))(inputs)
        x = Conv1D(24, 3, dilation_rate=1, padding='same', activation='relu')(x)
        x = LayerNormalization()(x)
        x = Conv1D(48, 3, dilation_rate=2, padding='same', activation='relu')(x)
        x = LayerNormalization()(x)
        x = Conv1D(96, 3, dilation_rate=4, padding='same', activation='relu')(x)
        x = LayerNormalization()(x)
        x = MaxPooling1D(2)(x)
        x = tf.keras.layers.Flatten()(x)  # GAP ëŒ€ì‹  Flatten ì‚¬ìš©
    
    # ê³µí†µ ë¶„ë¥˜ì¸µ
    x = Dense(128, activation='relu')(x)
    x = LayerNormalization()(x)
    x = Dropout(0.2)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    
    return Model(inputs=inputs, outputs=outputs)

# ë“€ì–¼ ë¸Œëœì¹˜ ëª¨ë¸ ìƒì„± (x, z ë¸Œëœì¹˜ë¡œ ë‹¤ì–‘í•œ íŠ¹ì§• ì¶”ì¶œ)
input_dim = X_train_scaled.shape[1]
model = create_dual_branch_model(input_dim, num_classes=21)

# ì»¤ìŠ¤í…€ F1 Score ë©”íŠ¸ë¦­ ì •ì˜
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=21, average='macro', name='f1_score', **kwargs):
        super().__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.average = average
        self.f1_score = self.add_weight(name='f1', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
        
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.argmax(y_pred, axis=-1)
        y_true = tf.cast(y_true, y_pred.dtype)
        
        # F1 score ê³„ì‚°ì„ ìœ„í•œ confusion matrix
        f1_value = tf.py_function(
            func=self._compute_f1,
            inp=[y_true, y_pred],
            Tout=tf.float32
        )
        
        self.f1_score.assign_add(f1_value)
        self.count.assign_add(1.0)
    
    def _compute_f1(self, y_true, y_pred):
        return f1_score(y_true.numpy(), y_pred.numpy(), average=self.average, zero_division=0)
    
    def result(self):
        return self.f1_score / self.count
    
    def reset_state(self):
        self.f1_score.assign(0.0)
        self.count.assign(0.0)

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=Adam(learning_rate=0.01),
    loss=SparseCategoricalCrossentropy(),
    metrics=['accuracy', F1Score(num_classes=21, average='macro')]
)

print("ëª¨ë¸ êµ¬ì¡°:")
model.summary()

# 6. í•™ìŠµ ì„¤ì • (ì½œë°±)
print("\n6. í•™ìŠµ ì½œë°± ì„¤ì • ì¤‘...")

# ì»¤ìŠ¤í…€ ëª¨ë‹ˆí„°ë§ ì½œë°±
class DetailedMonitoringCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super().__init__()
        self.best_val_loss = float('inf')
        self.best_val_accuracy = 0.0
        self.best_val_f1 = 0.0
        self.epochs_without_improvement = 0
        
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        
        val_loss = logs.get('val_loss')
        val_accuracy = logs.get('val_accuracy')
        val_f1 = logs.get('val_f1_score', 0.0)
        train_loss = logs.get('loss')
        train_accuracy = logs.get('accuracy')
        train_f1 = logs.get('f1_score', 0.0)
        
        # í˜„ì¬ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°
        current_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))
        
        print(f"\nğŸ“Š Epoch {epoch + 1} ê²°ê³¼:")
        print(f"   ğŸ”¸ Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Train F1: {train_f1:.4f}")
        print(f"   ğŸ”¸ Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}")
        print(f"   ğŸ”¸ Learning Rate: {current_lr:.2e}")
        
        # ê²€ì¦ F1 ì ìˆ˜ ê°œì„  ì²´í¬ (ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ)
        if val_f1 > self.best_val_f1:
            improvement = val_f1 - self.best_val_f1
            self.best_val_f1 = val_f1
            self.epochs_without_improvement = 0
            print(f"   âœ… ê²€ì¦ F1 ì ìˆ˜ ê°œì„ ! (+{improvement:.4f}) ğŸ¯")
        else:
            self.epochs_without_improvement += 1
            print(f"   âš ï¸  ê²€ì¦ F1 ì ìˆ˜ ê°œì„  ì—†ìŒ ({self.epochs_without_improvement}íšŒ ì—°ì†)")
        
        # ê²€ì¦ ì†ì‹¤ ê°œì„  ì²´í¬
        if val_loss < self.best_val_loss:
            improvement = self.best_val_loss - val_loss
            self.best_val_loss = val_loss
            print(f"   âœ… ê²€ì¦ ì†ì‹¤ ê°œì„ ! (ì´ì „ ëŒ€ë¹„ -{improvement:.4f})")
        
        # ê²€ì¦ ì •í™•ë„ ê°œì„  ì²´í¬
        if val_accuracy > self.best_val_accuracy:
            improvement = val_accuracy - self.best_val_accuracy
            self.best_val_accuracy = val_accuracy
            print(f"   âœ… ê²€ì¦ ì •í™•ë„ ê°œì„ ! (+{improvement:.4f})")
        
        print("-" * 60)

# EarlyStopping: ê²€ì¦ F1 ì ìˆ˜ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
early_stopping = EarlyStopping(
    monitor='val_f1_score',
    mode='max',  # F1 ì ìˆ˜ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
    patience=15,
    restore_best_weights=True,
    verbose=1
)

# ReduceLROnPlateau: ê²€ì¦ F1 ì ìˆ˜ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
reduce_lr = ReduceLROnPlateau(
    monitor='val_f1_score',
    mode='max',  # F1 ì ìˆ˜ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

# ëª¨ë‹ˆí„°ë§ ì½œë°±
monitoring_callback = DetailedMonitoringCallback()

callbacks = [early_stopping, reduce_lr, monitoring_callback]

# 7. ëª¨ë¸ í•™ìŠµ
print("\n7. ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

print(f"\nğŸš€ ë“€ì–¼ ë¸Œëœì¹˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
print(f"   ğŸ“ˆ ì´ ì—í¬í¬: 200 (ìµœëŒ€)")
print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: 128")
print(f"   ğŸ¯ ëª©í‘œ: 21ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)")
print(f"   âš–ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ì ìš© (ë¶ˆê· í˜• í•´ê²°)")
print(f"   â±ï¸  ì¡°ê¸° ì¢…ë£Œ: 15 ì—í¬í¬ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨")
print(f"   ğŸ”§ ëª¨ë¸ êµ¬ì¡°: X ë¸Œëœì¹˜(Dilated 1,2) + Z ë¸Œëœì¹˜(Dilated 4,8) + Transformer")
print(f"   ğŸŒŸ íŠ¹ì§•: ë“€ì–¼ Dilated Conv1D â†’ Positional Encoding â†’ Multi-Head Attention")
print("=" * 60)

history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=200,
    batch_size=128,
    class_weight=class_weight_dict,  # Macro F1 Score ìµœì í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
    callbacks=callbacks,
    verbose=0  # ì»¤ìŠ¤í…€ ì½œë°±ì´ ì¶œë ¥ì„ ë‹´ë‹¹í•˜ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
)

print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
print(f"   ğŸ“Š ì´ {len(history.history['loss'])} ì—í¬í¬ ì‹¤í–‰")
print(f"   ğŸ† ìµœì¢… ê²€ì¦ ì •í™•ë„: {max(history.history['val_accuracy']):.4f}")
print(f"   ğŸ¯ ìµœì¢… ê²€ì¦ F1 ì ìˆ˜: {max(history.history['val_f1_score']):.4f}")
print(f"   ğŸ’« ìµœì¢… ê²€ì¦ ì†ì‹¤: {min(history.history['val_loss']):.4f}")
print("=" * 60)

# 8. ëª¨ë¸ í‰ê°€
print("\n8. ëª¨ë¸ í‰ê°€ ì¤‘...")

# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
y_val_pred = model.predict(X_val_scaled)
y_val_pred_classes = np.argmax(y_val_pred, axis=1)

# ì •í™•ë„ ë° Macro F1 Score ê³„ì‚°
val_accuracy = accuracy_score(y_val, y_val_pred_classes)
val_macro_f1 = f1_score(y_val, y_val_pred_classes, average='macro')
val_weighted_f1 = f1_score(y_val, y_val_pred_classes, average='weighted')

print(f"ğŸ¯ ëŒ€íšŒ í‰ê°€ ì§€í‘œ (Macro F1 Score): {val_macro_f1:.4f}")
print(f"ğŸ“Š ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}")
print(f"âš–ï¸  Weighted F1 Score: {val_weighted_f1:.4f}")

# ë¶„ë¥˜ ë¦¬í¬íŠ¸ (í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„)
print("\nğŸ“‹ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„:")
print(classification_report(y_val, y_val_pred_classes))

# í•™ìŠµ ê³¡ì„  ì‹œê°í™” (Macro F1 Score í¬í•¨)
plt.figure(figsize=(20, 5))

plt.subplot(1, 4, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 4, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 4, 3)
plt.plot(history.history['f1_score'], label='Training F1 (Macro)')
plt.plot(history.history['val_f1_score'], label='Validation F1 (Macro)')
plt.title('Macro F1 Score (ëŒ€íšŒ í‰ê°€ ì§€í‘œ)')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.legend()

# í˜¼ë™ í–‰ë ¬
plt.subplot(1, 4, 4)
cm = confusion_matrix(y_val, y_val_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.savefig('C:/Users/jsy/Desktop/coretech/Dacon/smart/model/training_results.png', dpi=300, bbox_inches='tight')
plt.show()

# 9. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)
print("\n9. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ì´ë¯¸ í•™ìŠµëœ ìµœì  ëª¨ë¸ ì‚¬ìš©)
test_predictions = model.predict(X_test_scaled)
test_pred_classes = np.argmax(test_predictions, axis=1)

# 10. ì œì¶œ íŒŒì¼ ìƒì„±
print("\n10. ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")

submission_df['target'] = test_pred_classes

# ê²°ê³¼ ì €ì¥
output_path = 'C:/Users/jsy/Desktop/coretech/Dacon/smart/data/deeplearning_submission.csv'
submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

# ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ í™•ì¸
print(f"\nì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:")
unique, counts = np.unique(test_pred_classes, return_counts=True)
for cls, count in zip(unique, counts):
    print(f"í´ë˜ìŠ¤ {cls}: {count}ê°œ")

print("\n" + "=" * 60)
print("Conv1D + Transformer ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ!")
print("=" * 60)