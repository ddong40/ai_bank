"""
LightGBM ê¸°ë°˜ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸
- IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ìœ¼ë¡œ ì™„í™” ì²˜ë¦¬
- Stratified Splitìœ¼ë¡œ ë¼ë²¨ ê· í˜• ìœ ì§€
- LightGBMìœ¼ë¡œ 21ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° êµì°¨ ê²€ì¦ ì ìš©
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# ì‹œë“œ ê³ ì •
np.random.seed(42)

print("=" * 60)
print("LightGBM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ë¹„ì •ìƒ ì‘ë™ ë¶„ë¥˜ ëª¨ë¸")
print("=" * 60)

# 1. ë°ì´í„° ì ì¬ ë° ë¶„ë¦¬
print("\n1. ë°ì´í„° ì ì¬ ì¤‘...")
train_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/train.csv')
test_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/test.csv')
submission_df = pd.read_csv('C:/Users/jsy/Desktop/coretech/Dacon/smart/data/sample_submission.csv')

# X, y ë¶„ë¦¬
X = train_df.drop(columns=['target', 'ID'])
y = train_df['target']  # ì •ìˆ˜ ë¼ë²¨ ê·¸ëŒ€ë¡œ ìœ ì§€ (0~20)

print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"í”¼ì²˜ ìˆ˜: {X.shape[1]}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {len(y.unique())}")
print(f"í´ë˜ìŠ¤ ë¶„í¬:\n{y.value_counts().sort_index()}")

# 2. Stratified Split (ë¼ë²¨ ê· í˜• ìœ ì§€)
print("\n2. ë°ì´í„° ë¶„í•  ì¤‘...")
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]}ê°œ")
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape[0]}ê°œ")

# 3. ì´ìƒì¹˜ í´ë¦¬í•‘ (ì™„í™” ì²˜ë¦¬)
print("\n3. IQR ê¸°ë°˜ ì´ìƒì¹˜ í´ë¦¬í•‘ ì ìš© ì¤‘...")

def apply_iqr_clipping(X_train, X_val, X_test):
    """IQR ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ í´ë¦¬í•‘í•˜ëŠ” í•¨ìˆ˜"""
    X_train_clipped = X_train.copy()
    X_val_clipped = X_val.copy()
    X_test_clipped = X_test.copy()
    
    clip_info = {}
    
    for column in X_train.columns:
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ IQR ê³„ì‚°
        Q1 = X_train[column].quantile(0.25)
        Q3 = X_train[column].quantile(0.75)
        IQR = Q3 - Q1
        
        # ê²½ê³„ê°’ ì„¤ì •
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        clip_info[column] = (lower_bound, upper_bound)
        
        # ëª¨ë“  ë°ì´í„°ì…‹ì— í´ë¦¬í•‘ ì ìš©
        X_train_clipped[column] = X_train[column].clip(lower_bound, upper_bound)
        X_val_clipped[column] = X_val[column].clip(lower_bound, upper_bound)
        X_test_clipped[column] = X_test[column].clip(lower_bound, upper_bound)
    
    return X_train_clipped, X_val_clipped, X_test_clipped, clip_info

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
X_test = test_df.drop(columns=['ID'])

# í´ë¦¬í•‘ ì ìš©
X_train_clipped, X_val_clipped, X_test_clipped, clip_info = apply_iqr_clipping(
    X_train, X_val, X_test
)

print(f"í´ë¦¬í•‘ ì™„ë£Œ. ì²˜ë¦¬ëœ í”¼ì²˜ ìˆ˜: {len(clip_info)}")

# 4. ì •ê·œí™” (MinMax Scaling)
print("\n4. MinMaxScalerë¥¼ ì´ìš©í•œ ì •ê·œí™” ì¤‘...")
scaler = MinMaxScaler()

# í›ˆë ¨ ë°ì´í„°ì— fit, ëª¨ë“  ë°ì´í„°ì— transform (0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§)
X_train_scaled = scaler.fit_transform(X_train_clipped)
X_val_scaled = scaler.transform(X_val_clipped)
X_test_scaled = scaler.transform(X_test_clipped)

print("MinMax ì •ê·œí™” ì™„ë£Œ (ë²”ìœ„: 0-1)")

# 5. LightGBM ëª¨ë¸ êµ¬ì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
print("\n5. LightGBM ëª¨ë¸ êµ¬ì„± ì¤‘...")

# LightGBM ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
lgb_params = {
    'objective': 'multiclass',
    'num_class': 21,
    'metric': 'multi_logloss',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.1,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'random_state': 42,
    'n_jobs': -1
}

print("LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
for key, value in lgb_params.items():
    print(f"  {key}: {value}")

# ì…ë ¥ ì°¨ì› í™•ì¸
input_dim = X_train_scaled.shape[1]
print(f"\nì…ë ¥ ì°¨ì›: {input_dim}")
print(f"í´ë˜ìŠ¤ ìˆ˜: 21")

# 6. êµì°¨ ê²€ì¦ ì„¤ì •
print("\n6. Stratified K-Fold êµì°¨ ê²€ì¦ ì„¤ì • ì¤‘...")

# Stratified K-Fold ì„¤ì • (í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤)
n_folds = 5
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

print(f"êµì°¨ ê²€ì¦: {n_folds}-Fold Stratified K-Fold")
print("ê° í´ë“œë§ˆë‹¤ í´ë˜ìŠ¤ ë¶„í¬ê°€ ê· ë“±í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.")

# 7. LightGBM ëª¨ë¸ í•™ìŠµ ë° êµì°¨ ê²€ì¦
print("\n7. LightGBM ëª¨ë¸ êµì°¨ ê²€ì¦ í•™ìŠµ ì‹œì‘...")

print(f"\nğŸš€ LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
print(f"   ğŸ“ˆ êµì°¨ ê²€ì¦: {n_folds}-Fold")
print(f"   ğŸ¯ ëª©í‘œ: 21ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ (ìµœê³  F1 ìŠ¤ì½”ì–´)")
print(f"   ğŸ”§ ëª¨ë¸: LightGBM Gradient Boosting")
print(f"   ğŸŒŸ íŠ¹ì§•: íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…")
print("=" * 60)

# êµì°¨ ê²€ì¦ ê²°ê³¼ ì €ì¥ìš©
cv_scores = []
cv_f1_scores = []
oof_predictions = np.zeros(len(X_train_scaled))  # Out-of-fold ì˜ˆì¸¡ê°’
test_predictions = np.zeros((len(X_test_scaled), 21))  # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ (í™•ë¥ )

fold_num = 1
for train_idx, val_idx in skf.split(X_train_scaled, y_train):
    print(f"\nğŸ“Š Fold {fold_num}/{n_folds} í•™ìŠµ ì¤‘...")
    
    # ë°ì´í„° ë¶„í• 
    X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    
    # LightGBM ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_fold_train, label=y_fold_train)
    val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)
    
    # ëª¨ë¸ í•™ìŠµ
    model = lgb.train(
        lgb_params,
        train_data,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        num_boost_round=1000,  # ìµœëŒ€ 1000 ë¼ìš´ë“œ (ë”¥ëŸ¬ë‹ì˜ epochì™€ ìœ ì‚¬)
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=True),  # 100 ë¼ìš´ë“œ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
            lgb.log_evaluation(period=50)  # 50 ë¼ìš´ë“œë§ˆë‹¤ ì„±ëŠ¥ ì¶œë ¥
        ]
    )
    
    # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
    val_pred_proba = model.predict(X_fold_val, num_iteration=model.best_iteration)
    val_pred_classes = np.argmax(val_pred_proba, axis=1)
    
    # Out-of-fold ì˜ˆì¸¡ê°’ ì €ì¥
    oof_predictions[val_idx] = val_pred_classes
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (í‰ê· ì„ ìœ„í•´ ëˆ„ì )
    test_pred_proba = model.predict(X_test_scaled, num_iteration=model.best_iteration)
    test_predictions += test_pred_proba / n_folds
    
    # ì„±ëŠ¥ í‰ê°€
    fold_accuracy = accuracy_score(y_fold_val, val_pred_classes)
    fold_f1 = f1_score(y_fold_val, val_pred_classes, average='macro')
    
    cv_scores.append(fold_accuracy)
    cv_f1_scores.append(fold_f1)
    
    print(f"   âœ… Fold {fold_num} ì™„ë£Œ:")
    print(f"      ğŸ“ˆ ì •í™•ë„: {fold_accuracy:.4f}")
    print(f"      ğŸ¯ F1 ì ìˆ˜: {fold_f1:.4f}")
    print(f"      ğŸŒŸ ìµœì  ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ: {model.best_iteration}")
    
    fold_num += 1

# ì „ì²´ êµì°¨ ê²€ì¦ ê²°ê³¼
print(f"\nğŸ‰ êµì°¨ ê²€ì¦ ì™„ë£Œ!")
print(f"   ğŸ“Š í‰ê·  ì •í™•ë„: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
print(f"   ğŸ¯ í‰ê·  F1 ì ìˆ˜: {np.mean(cv_f1_scores):.4f} Â± {np.std(cv_f1_scores):.4f}")
print(f"   ğŸ† ìµœê³  ì •í™•ë„: {max(cv_scores):.4f}")
print(f"   ğŸ’« ìµœê³  F1 ì ìˆ˜: {max(cv_f1_scores):.4f}")
print("=" * 60)

# 8. Out-of-Fold ì˜ˆì¸¡ í‰ê°€
print("\n8. Out-of-Fold ì˜ˆì¸¡ í‰ê°€ ì¤‘...")

# Out-of-fold ì˜ˆì¸¡ í‰ê°€ (ì „ì²´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ êµì°¨ ê²€ì¦ ì˜ˆì¸¡)
oof_accuracy = accuracy_score(y_train, oof_predictions)
oof_f1 = f1_score(y_train, oof_predictions, average='macro')

print(f"Out-of-Fold ì •í™•ë„: {oof_accuracy:.4f}")
print(f"Out-of-Fold F1 ì ìˆ˜: {oof_f1:.4f}")

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\nOut-of-Fold ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_train, oof_predictions))

# êµì°¨ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 5))

# í´ë“œë³„ ì„±ëŠ¥ ì‹œê°í™”
plt.subplot(1, 3, 1)
folds = range(1, n_folds + 1)
plt.bar(folds, cv_scores, alpha=0.7, label='Accuracy', color='skyblue')
plt.axhline(np.mean(cv_scores), color='red', linestyle='--', label=f'Mean: {np.mean(cv_scores):.4f}')
plt.title('Cross-Validation Accuracy')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.legend()

plt.subplot(1, 3, 2)
plt.bar(folds, cv_f1_scores, alpha=0.7, label='F1 Score', color='lightgreen')
plt.axhline(np.mean(cv_f1_scores), color='red', linestyle='--', label=f'Mean: {np.mean(cv_f1_scores):.4f}')
plt.title('Cross-Validation F1 Score')
plt.xlabel('Fold')
plt.ylabel('F1 Score')
plt.ylim(0, 1)
plt.legend()

# Out-of-Fold í˜¼ë™ í–‰ë ¬
plt.subplot(1, 3, 3)
cm = confusion_matrix(y_train, oof_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Out-of-Fold Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.savefig('C:/Users/jsy/Desktop/coretech/Dacon/smart/model/training_results.png', dpi=300, bbox_inches='tight')
plt.show()

# 9. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (êµì°¨ ê²€ì¦ ì•™ìƒë¸”)
print("\n9. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (êµì°¨ ê²€ì¦ìœ¼ë¡œ ì´ë¯¸ ê³„ì‚°ë¨)
test_pred_classes = np.argmax(test_predictions, axis=1)

print(f"í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ: {len(test_pred_classes)}ê°œ ìƒ˜í”Œ")
print("êµì°¨ ê²€ì¦ ì•™ìƒë¸” ì˜ˆì¸¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# 10. ì œì¶œ íŒŒì¼ ìƒì„±
print("\n10. ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")

submission_df['target'] = test_pred_classes

# ê²°ê³¼ ì €ì¥
output_path = 'C:/Users/jsy/Desktop/coretech/Dacon/smart/data/deeplearning_submission.csv'
submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

# ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ í™•ì¸
print(f"\nì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:")
unique, counts = np.unique(test_pred_classes, return_counts=True)
for cls, count in zip(unique, counts):
    print(f"í´ë˜ìŠ¤ {cls}: {count}ê°œ")

print("\n" + "=" * 60)
print("LightGBM ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ!")
print("=" * 60)